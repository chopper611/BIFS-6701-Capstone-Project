--- SOURCE: Week 1 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week 1 Lecture - Introduction to Bioinformatics and Algorithms

Bioinformatics 

Bioinformatics has been described as a “mutually beneficial collaboration between biology and computer science”.  The problems faced by a modern biologist (or “life scientist”) frequently involve complex data sets – in terms of both the type of data and the size of the data set – and this in turn provides exciting new opportunities for synergistic activities with computer scientists.

Defining the problem

Often one of the biggest challenges you will face as a bioinformaticist may surprise you – often the most difficult part of a project is determining the accurate definition of the problem you are trying to solve.  In computer science that is known “gathering requirements” or figuring out exactly what the algorithm you are about to write needs to do, and in what order.  In other words, what are the goals of the code?  What steps do you have to take to accomplish them?  For example, let’s say you need to write an algorithm to reverse-complement a DNA sequence.  Sounds simple, right?  Let’s see if we can define the requirements:

Take an input DNA sequence

Reverse-complement the sequence

Provide the reverse-complement sequence as the output

The first step – taking an input – and the last step – providing the output – are very easy.  What, exactly, does step 2 mean?  A programmer might not know, so the biologist has to explain clearly (this is why the best bioinformaticist is someone with both biological and computer science backgrounds).  Let’s walk through it:

Let’s start with this very simple sequence:  ATGGCC.  It might be better, though, to write the sequence out formally like this:  5’-ATGGCC-3’.  Recall that DNA is an anti-parallel double-helix.  This means that the strands run in opposite directions to each other.  So the actual DNA molecular at this point would look like this:

5’-ATGGCC-3’

3’-TACCGG-5’

We usually only provide the sequence in the relevant orientation – that is, the strand that is in the “gene” orientation.  Hence, we would provide the one with the START codon (the ATG) only, which is what we provided initially (ATGGCC).

The reverse-complement of a sequence is the complementary sequence, reversed.  So for the above example (ATGGCC) the complement is the complementary strand, reversed (so you are still reading it in the 5’ to 3’ direction):  GGCCAT.

So the requirements could be updated to be the following:

Take an input DNA sequence

Reverse-complement the sequence

Take the complement of the given sequence

Reverse the complement so it is displayed in 5’ to 3’ orientation

Provide the reverse-complement sequence as the output

Solving the problem

Once you have the requirements, it’s easy to start blocking out your algorithm in what we refer to as “pseudocode”.  In fact, a good set of requirements (like the above) is very close to being pseudocode.

Pseudo code is a representation of code which can be easily understood by anyone with minimal programming knowledge – an organized and complete sequence of steps that have to be taken to solve the problem, written in plain English.  This differs from an algorithm in that there is no specific syntax or formalized variable structure in place, so it can’t actually be run on a computer.  So why would anyone use pseudocode?  Here’s why:

Pseudocode improves the readability of any approach and makes it easy for anyone to understand what the algorithm is doing

Pseudocode acts as an outline for the actual algorithm that will be written – and can be used as the basis for writing the algorithm in any language.

Pseudocode can be used as the “comment” blocks in the actual algorithm to help make sure that, again, what the code should doing at any particular line is easily understood.

Now let’s take our requirements above and convert them into pseudocode.  The basic process is simply, providing you have already arranged the tasks that the algorithm must perform in an orderly sequential fashion.  I’ll also use a separator character at the beginning of each line so that when I want to convert the pseudocode to an algorithm I can use the pseudocode as the comment.  Since most (but not all) programming languages use “#” as the comment indicator, I’ll use that and start with some basic information:

# The purpose of this code is to take an input sequence and

# reverse-complement it, then provide the output.

Notice that I used multiple lines – you can have as many comments as you like.  In fact, the more, the merrier!  It will make understand not only what the code is supposed to do, but how it is doing it, much easier for everyone – including yourself when you revisit the code years later.

Now let’s start adding lines:

# The purpose of this code is to take an input sequence and

# reverse-complement it, then provide the output.

#

# Read In the Input Sequence

Notice that I used an extra “#” to separate the “comments” from the “pseudocode”.  This makes it easy to read!

I could also have written this:

# The purpose of this code is to take an input sequence and

# reverse-complement it, then provide the output.

#

# Read In the Input Sequence and store it in a string called input_sequence

A string is a type of variable this is exactly what it sounds like – a series of values.  A DNA sequence is very readily stored in a string.  This is more specific and more detailed, although chances are that anyone taking the version without “and store it in a string” would have known to do that with the input.  It’s something you just do when you read in data – cast it in a variable of some sort.  It’s also a good idea to decide on what your variable names could/should be up front – and use good variable names, like “input_sequence” that describe exactly what they are.

Let’s continue: 

# The purpose of this code is to take an input sequence and

# reverse-complement it, then provide the output.

#

# Read In the Input Sequence and store it in a string called input_sequence

#

# Now get the complement of the string: 

# 

#      Read each base in the string and replace it with the appropriate complementary base

#      (Alternately create a new string variable and populate it with the complementary base)

Notice what I did there?  Not knowing for sure which would be easier I just put down the ideas I had for implementing that step.  It’s always a good idea to put your thoughts down like that – later, when you actually write an algorithm in a programming language, you can decide which method is “best”.  Let’s keep going:

# The purpose of this code is to take an input sequence and

# reverse-complement it, then provide the output.

#

# Read In the Input Sequence and store it in a string called input_sequence

#

# Now get the complement of the string: 

# 

#      Read each base in the string and replace it with the appropriate complementary base

#      (Alternately create a new string variable and populate it with the complementary base)

#

# Output the reverse-complemented sequence

#

The pseudocode now has all the steps required to meet the stated requirements.

Even though pseudocode isn’t “real code”, it forms the basis for it.  As such, you should try to follow some basic guidelines that will help you formalize things when you do start coding:

Use good naming conventions. Use simple and consistent naming for variables in your pseudocode, so that the variable name tells you and anyone else reviewing your code what that variable holds.

Use consistent upper and lower case naming conventions.  Some recommend using CamelCase for a routine and lower case for variables (e.g. InvertSequence as a routine, input_sequence as a variable).  

Use as much detail as you can to describe what is going to happen in the actual code. This will help you write the code, and others understand the code.

Use standard programming structures such as ‘if-then-else’, ‘for’, ‘while’, etc to help indicate what the code will be doing.

After you’ve finished, go back and re-read your code and make sure it tells a consisten, linear story.

DO NOT write the pseudo code in a complete programmatic style. It needs to be simple so that anyone, even a non-programmer, can understand what your pseudocode algorithm is doing.

Writing the Algorithm

Your pseudocode is, in many sense, your “algorithm”.  It is the list of steps, in order, that must occur in order to solve the problem/task you have defined in your requirements.    Actually writing it in code is another matter, however.  Depending on your experience, you may choose to write the code in R, python, perl, java, or any other language.  Even bash has enough data processing commands (e.g. sed and awk) to perform many bioinformatics tasks!  Any of these languages is fine – they share many similarities in their general structure, so choose one that you like.  Most bioinformatics have moved towards python, so if you are starting out fresh that’s a great program to learn.

While we’re talking about programming languages, a word about IDEs.  IDE stands for Integrated Development Environment – essentially an IDE is a program that helps you write code using features such as autocomplete of terms (including functions and variables), syntax checking, and multiple windows that allow you to see your code as well as any variables you’ve declared – and their contents, etc. etc.  I strongly recommend that you consider using an IDE for development – it will make your life much easier!  If you are planning on using python take a look at Spyder, which is a fantastic IDE for python development.  Also, please consider Anaconda, which lets you set up multiple python environments (e.g. for different versions of python, or different libraries within python).  Similarly there are IDEs for R (RStudio) and every other language out there.

When you are writing your code, don’t be afraid to use a brute force approach to solve the problem – you may know that there is a “better, more elegant” solution, but often it’s fine to do something inelegant and fast in your first implementation.  You can always go back and optimize the code later (and we’ll talk about optimization a bit more next week with BigO).  

Also be sure to comment the heck out of your code!  I like to say that good code is 50% code and 50% comments….make sure you and anyone else reviewing your code in a few years knows exactly what you are trying to do and how.  If you use python, one of the first comments in your code should include the python version information – python 2.7 was very popular for a very long time, but it is finally deprecated, and python 3.5 (and higher) have replaced it.  If you try to run python 2.7 code in a 3.5 environment (or vice versa), you will quickly see that the code simply won’t work – so make sure to indicate the correct version!

Once the basic algorithm is written there is still work to do in eliminating “bottlenecks” – e.g., how efficient is the algorithm, how much “noise” is there in the data, and how meaningful are the results…this last one is often the most perplexing and difficult to deal with, since the data comes from one domain (biology) and the processing comes from a second domain (computer science), and the results are then fed back to the first domain.  From personal experience I've seen that this last problem is most easily solved when the “domain experts” – the people with the biology and/or computer experience – are one and the same.  And when working in a team environment, it's still optimal to have overlap – the biologist should understand the basics of an algorithm even if they can't write it themselves, and the programmers should understand some of the basics of biology!

Working with Data Structures

Equally important to the problem of how to analyze the data (the algorithm) is how to structure the input data. You can organize data in many different ways, some of which are more useful than others.  When data is collected and stored in a logical, pre-defined manner the result is called the “Data structure”.  There are many different existing data structures you will run into in bioinformatics, including:

FASTA

GENBANK

VCF

SAM/BAM

XML

Let’s take a look at one of the simplest examples – a FASTA file.  Originally developed to work with the FASTA sequence alignment software, a FASTA (pronounced “fast A”) file is very easy to understand.  Like most of the formats above, it’s a plain text file (although the file extension will be *.fasta, or *.fa).  FASTA files can contain one or more sequences, where each sequences has two lines:  the header line, and the data line.  The following example has 10 sequences:

>seq1
KYRTWEEFTRAAEKLYQADPMKVRVVLKYRHCDGNLCIKVTDDVVCLLYRTDQAQDVKKIEKFHSQLMRLME LKVTDNKECLKFKTDQAQEAKKMEKLNNIFFTLM
>seq2
EEYQTWEEFARAAEKLYLTDPMKVRVVLKYRHCDGNLCMKVTDDAVCLQYKTDQAQDVKKVEKLHGK
>seq3
MYQVWEEFSRAVEKLYLTDPMKVRVVLKYRHCDGNLCIKVTDNSVCLQYKTDQAQDVK
>seq4
EEFSRAVEKLYLTDPMKVRVVLKYRHCDGNLCIKVTDNSVVSYEMRLFGVQKDNFALEHSLL
>seq5
SWEEFAKAAEVLYLEDPMKCRMCTKYRHVDHKLVVKLTDNHTVLKYVTDMAQDVKKIEKLTTLLMR
>seq6
FTNWEEFAKAAERLHSANPEKCRFVTKYNHTKGELVLKLTDDVVCLQYSTNQLQDVKKLEKLSSTLLRSI
>seq7
SWEEFVERSVQLFRGDPNATRYVMKYRHCEGKLVLKVTDDRECLKFKTDQAQDAKKMEKLNNIFF
>seq8
SWDEFVDRSVQLFRADPESTRYVMKYRHCDGKLVLKVTDNKECLKFKTDQAQEAKKMEKLNNIFFTLM
>seq9
KNWEDFEIAAENMYMANPQNCRYTMKYVHSKGHILLKMSDNVKCVQYRAENMPDLKK
>seq10
FDSWDEFVSKSVELFRNHPDTTRYVVKYRHCEGKLVLKVTDNHECLKFKTDQAQDAKKMEK

It looks like the file wraps the text, but that’s only because of the limitations of the text viewer we are using (Word).  Remember that each sequence has only two lines, which means there are only two carriage return/line feeds – one at the end of the header, and one at the end of the sequence.  

The header contains any useful naming information – and you’ll eventually run into some crazy names including all sorts of “metadata”.  That’s because FASTA has no actual structure for storing anything other than the name and the sequence, so developers often pack the name with all sorts of goodies.

A Quick Aside:  Text Editors:

While I’m talking about plain text, let me point out that many people confuse Word Processors as Text Editors.  They are not at all the same!  Microsoft Word files are very highly structured, and if you load a text file into Word and save it as a Word file, you won’t be able to use it as a data file.  Go ahead and try opening a Word file up in a real text editor (I recommend Sublime Text or BBEdit) that shows you all the guts of the file!  After that, for something a bit more domain appropriate, use Sublime Text or BBEdit to look at a FASTA or VCF or CLUSTAL file – you should be able to find some on the internet.  And keep a real text editor (Sublime Text or BBEdit) handy; you’ll need it as you work with bioinformatics data files!

Sometimes the data structure is defined by the programming language, but in most cases you may find it easier to define your own input/output data structures based on both the data itself and the programming algorithm AND language combined.  And don't worry if you find yourself wanting to rewrite everything a few weeks after you’ve started – even the best programmers will design a structure (or an algorithm) and then go back later and tweak things when they see where they break, or aren't efficient.  This is all part of the iterative process of software development!

ON YOUR OWN:  Do some research on your own – you should know the difference between a table, a database, a plain-text file, and an XML format file.



--- SOURCE: Week 2 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week 2 Lecture – Algorithms & Algorithm Complexity

ALGORITHMS 

The real problem in any bioinformatics programming exercise is defining the algorithm for working with the data structure (and again, don't take this too literally – you'll find that it's very common to go back and forth, redefining the data structure while you are developing an algorithm). 

As you learned last week, an algorithm is basically a finite, well-defined set of instructions for accomplishing a task. An algorithm starts at an initial state and ends on a defined end-state. You write algorithms all the time, in your head – some are algorithms for preparing dinner (the initial state is a pile of ingredients, and the end state is a meal), others get you to work (the initial state is you at home with some means of transportation available to you, and the end state is you arriving at work). Computer algorithms have to be very precisely defined (computers will do exactly what you tell them to) - you need to define things like the input values (data structure) as well as every step used to transform that input into the desired output. 

Here are just a few common types of code structures you can use when building an algorithm:

-  Linear algorithms are simple series of sequential steps 

-  Loop algorithms are sets of steps that repeat until a specific condition is met 

-  Nested Loop algorithms are a loop containing another, different loop 

-  Branching algorithms consist of a number of sets where different conditions can result in different paths, or processing steps, being taken 

Loops are very common elements in programming. Here are some examples, written in pseudocode (recall from last week that pseudocode doesn’t actually run in any language, but rather is written in a way that humans can read and understand): 

FOR loop: 

for each X of seq [1,3,5,7,9]

Print x 

end 

This algorithm defines seq as the series of number 1, 3, 5, 7, and 9, and then prints each of them until there is nothing left to print.  The condition that terminates the FOR loop is completion of a task for a specific set of values in the data set defined by seq. 

There are other ways of defining the number of loops, depending on the language you are writing and executing the code in. For example: 

for [a to b] 

<statements> 

Here you might have the statements include incrementing the value of variable a by 1, so that the loop terminates once that value has reached the value of variable b. 

WHILE loop: 

While (logical condition) do 

<Loop body >

For the WHILE loop, you have to define a condition (for example, X<Y), and the loop will continue to do the code in the “loop body” as long as that condition is met. 

Branching code:

An example of branching code is the IF THEN ELSE statement: 

if (logical condition) 

then 

(statements) 

else 

(other statements) 

endif 

Here you again define a logical condition, but there are two ways the code can go – if the logical condition is met, the THEN code (which can be one line or thousands) is executed. If the logical condition is not met, the ELSE code is executed. This is also sometimes called a CONDITIONAL statement. 

You may have noticed by now that I have been doing a lot of strange indenting in the code snippets above. This indentation is a useful convention many programmers use to help separate where a loop or branch begins and ends – something you'll find very valuable when trying to read through a few hundred lines of code!  For some languages, like python, the indent structure is very rigorously controlled and required; for others it’s just for your own ease of use.

Other things you'll need to be familiar with when starting to write code include: 

Variable Assignment – basically assigning values to variables so that they can be processed, e.g: 

X = 2 (setting a hard value to a variable) 

X = B (giving a variable the current value of yet another variable) 

Input_sequence = $1 (assigning the first string after the program execution command to a string variable)

Basic Math – doing math with variables, e.g: 

x = a + b

x = a/b^2 

Again, the exact way you write these various steps is different for each programming language – each language has its own syntax, or grammar, for composing a statement. 

Let’s take a look at an example of a piece of code that uses some of these structures.  This will be an actual piece of executable code, so you’ll notice that there is a specific syntax and structure used to the code:

Code Example:  python 3.5:   Counting the # of occurrences of a base in a DNA sequence

You may already have learned some python; don’t worry if you haven’t – we will walk through this simple example together:

Line 1 begins with a hash symbol (#).  This is the comment indicator for python; any lines beginning with the hash symbol are not run.  In this case, since the hash is followed by the “!/bin/python”, it tells the interpreter software (and the user) that this is to be run with in python, and that the binary for python is installed at “/bin/python” – which is the default location for any linux-based operating system.

NOTE:  this code is written for python 3.5 or higher; I’m using python 3.8 on MacOS.  Python 2.5 was deprecated on Jan 1 2020, so you should not use it unless you have to run someone else’s code.

Line 2 is a spacer; I like to leave spaces to separate sections of code.

Lines 3-8 define a function.  For this example, the function count_v2 is the only function in the code, and it is in fact the most important bit of the algorithm – the part that counts the number of occurrences of the cysteine residues in the sequence.  Note that the function uses variables that are defined later in the code; that’s okay as long as anything that references the function occurs after the function in the code.  So if you wanted to move just the variables up higher, above the function, that would work just fine.

So what does our function do?  The first line, def count_v2(dna,base): has several components:

def tells python that we are defining a function

count_v2 is the name of the function

dna and base are the required input variables for the function

We end the line with a colon to indicate that the next lines are the actual code for the function.

Line 4 defines a simple counter variable (i) and initializes it by setting it to zero.

Line 5 starts a for loop.  The “c” is a variable for the loop; dna is a pre-defined variable that is established in line 10.  So what line 5 is doing is saying is that for every base in the variable dna….and then it does the rest of the lines in the for loop.  The “c” is where python temporarily stores each base from the variable dna while it is doing the loop – so that the first time through the loop, c is equal to “A”; the next time the loop runs c is equal to “T”, etc.  Note the colon at the end of the line!

Line 6 uses a comparator to see if the current value of variable c is equal to the defined value of base we are looking for.  Again there is a colon at the end of the line, because there is something to do if the condition is met – and that is done on line 7.

Line 7 increments the counter we set up in line 4.  So every time the variable c finds a match in base the counter is incremented – in this case, we are counting how many “C” residues we have encountered.

Line 8 tells the routine that there’s nothing else to and ends the routine, returning the value of “i” (the counter).

Line 9 is just another spacer line.

Line 10 is where the user can enter the value for the variable dna.  This is just a short sequence.  In practice, it would be smarter to have the program read in a text file so that it would be easy to change the string, rather than editing the code itself.

Line 11 defines the variable base, telling the program which base we are looking for.  Again there are smarter ways of doing this – for example, we could just have the algorithm count the frequency of all four bases by putting our function inside a loop that changes the value of base.  Alternately if we run this from the command line you can use input flags to tell the algorithm which base to look for.

Line 12 is actually the second most important part of this algorithm (after lines 3-8, the function count_v2).  In line 12 we invoke the function, provide the variables, and return the answer back as a new variable, “n”.

Line 13 is another spacer.

Line 14 is where we print out the answer using python’s built-in print command.  This outputs the answer to the standard output (the terminal).

We can make our code easier to follow through the addition of more comment lines.  Take a look at this updated version of the code – it should be much easier to read what’s going on now that I’ve added several comment lines:

While I was adding comments, I moved the variables to the top to make it easier to modify them.

Python is probably the single most important language used in bioinformatics - – I strongly recommend that you become familiar with it.  Here are some good resources – all free to individual users:

Anaconda:  allows you to create multiple independent sandboxed python installs

Anaconda Navigator:  graphical user interface for working with python and other data science applications

Spyder:  A great IDE for building python programs

Bioinformatics Examples in python:  http://hplgit.github.io/bioinf-py/doc/pub/html/index.html

Code Example:  R:

Now let’s do a simple example in R.  As before, you may already have learned some R; don’t worry if you haven’t.  We’re going to do things a bit differently here – rather than writing the entire algorithm from scratch, we’re going to take advantage of pre-existing libraries that provide functions.  These sorts of libraries exist for python too – it’s a good thing to keep in mind, that you don’t have to reinvent the wheel; chances are someone out there has already written functions or libraries that will do at least part of what you need, so it’s worth time looking at what’s out there before coding.

As before, we’ll walk through this code together:

For this example, I’m using the RStudio IDE to help me with my code.  It has features like autocomplete and variable exploration that are extremely useful.  The window above only shows the code window.

Line 1 is just a comment line where I describe the program.  Other useful things you can incorporate here are your name, the date, the version of the software, your contact info, etc – that way if you let other people use your code they’ll know if they have the latest copy, or how to contact you.

Lines 3  is the comment telling us what the next block of lines do.

Lines 4-6 check to see if Bioconductor is required and install it if you haven’t already done so.

NOTE:  Bioconductor is a set of extremely useful packages for bioinformatics in R – if you are going to use R for bioinformatics development, you simply have to check out Bioconductor – it will save you an incredible amount of time.

Line 8 is the comment line for line 9.

Line 9 uses Bioconductor’s own package manager to install a package from within Bioconductor called “Biostrings”.  This package/library has many nice functions – we’ll take a look at a few.

Line 11 tells R that we need to actually use the “Biostrings” library.  You can install many different libraries in your R installation, and you only need to install them once.  Whenever you are writing code that needs that library, you simply use the “library” command as in line 11 to load it.

Line 14 uses the Biostrings “DNAString” command to create the variable “myDNA” containing the sequence displayed.  You could do this without Biostrings, but the variable type and subsequent processing would be very different than what we are going to do here.  Biostrings is saving us a lot of time by creating the variable as a class of variable that it can manipulate intelligently.

Line 16 does all of the frequency counting.  Not only does it count all of the “C” residues, as our python program does, but it counts the frequency of every possible type of residue for a DNA sequence!

Line 17 simply tells R to display the variable freq_count.  The output is shown in RStudio’s output window, and looks like this:

> freq_count

 A  C  G  T  M  R  W  S  Y  K  V  H  D  B  N  -  +  . 

14  7  4 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0

In addition to the 4 nitrogenous bases, this displays the IUPAC code for ambiguous bases.  Since there are none in this example, they are all listed as zero.  

Line 19 uses yet another function from Biostrings to provide the reverse of the sequence.  You can imagine how much easier this is than writing the function yourself!  The answer as seen in Line 20 is this:

> reverse_myDNA

  35-letter "DNAString" instance

seq: ACATAACCCAGCAAGAATTAATTACTTACGTTGTA

Line 22 reverse-complements the sequence:

> rev_comp_myDNA

  35-letter "DNAString" instance

seq: TGTATTGGGTCGTTCTTAATTAATGAATGCAACAT

Line 25 translates the sequence:

> myDNA_translation

  11-letter "AAString" instance

seq: MLHSLIKNDPI

We did the same thing in our R example as we did in our python example (count the number of “C” residues in a sequence) – and much more.  Using the Bioconductor package “Biostrings” reduced the counting portion of our script to a single line!  There are libraries for all sorts of functions that you can install for python to help you in a similar fashion.

R is a common “scripting” language (some people say it’s not programming at all; I tend to disagree).  It’s very useful for statistical analysis (and we’ll talk about some of those, like LDA and PCA, later in the course); it has some of the best graph plotting capabilities of any software available.  Chances are you may find yourself using it some day – here are some good R resources:

R:  The main download and instructions page for R

RStudio:  A very helpful IDE (Integrated Development Environment) for R.  The desktop (non-pro) version is free, and fully functional!

Bioconductor:  A very comprehensive library of functions for bioinformatics in R

R for Biologists:  A nice set of tutorials for using R.  Includes genomics examples.

Recursive vs Iterative Algorithms

There are two general approaches to repetitive (non-linear) algorithms, iteration and recursion. In general, any given problem is likely to be solvable using either one of these – and in practice you should focus more on solving the problem than on adhering to a specific style of solution. Remember that sometimes the hardest part is to know exactly what the problem is! 

RECURSION – a repetitive process in which the algorithm (usually a subroutine or call) calls upon itself. These may have a higher overhead and require more time and memory, but may be a simpler algorithm (which ultimately makes them easier to debug). Recursive algorithms are well suited for recursive data structures – trees and graphs, etc. 

Recursive algorithms can be used to simplify a large problem by breaking it into smaller parts and solving them recursively. The general approach is to take a general solution to a “base case” (e.g. something that gives you an answer to a simple problem) and then use that same approach to solve larger problems broken into chunks (the general case). 

EXAMPLE: A very simple example would be searching an array for a specific value – the general approach is to take the first value (or element) from the array and test to see if it is the value you are looking for. If it isn't, get the next value and repeat the process (get a value, test it, then repeat) until you either find the value you are looking for, or find that the value isn't present anywhere in the array. 

EXAMPLE: Another example is an algorithm to calculate factorials. Recall that a factorial is n!, where n is a number. To calculate the factorial you use: 

n x (n-1) x (n-2) x (n-3)... until n is 1 (so that last step is n = 1) 

This is essentially a recursive algorithm – many repeating subunits of the same calculation. 

Recursive algorithms are used when you have a problem that can be broken into many repeating smaller problems, all of which use exactly the same method to be solved. 

ITERATION – an algorithm that repeats until a task is done, usually using loops and counters. Iterative algorithms frequently are more efficient, in that they run faster and use less memory. An iterative algorithm might break the problem into a for-next loop – for example, when solving the factorial above, you would write a loop that does the calculation for “n” steps. 

The take home message – writing a program to solve a problem involves knowing which steps you take when you do it yourself, and then writing those steps in a fashion that the 

computer can repeat them and get the correct answer. Iterative and Recursive algorithms are two general design ideologies, or approaches, to solving repetitive problems – again, sometimes the best code is the one that gets the answer, not the cleanest or the purest! 

ALGORITHM COMPLEXITY 

Developers routinely analyze algorithms they've written for two reasons (other than the obvious need to fix bugs): 

-  optimization in terms of time of execution 

-  optimization in terms of space utilization 

First we're going to talk about time of execution, which is affected by several things, including: 

-  the hardware running the program 

-  the language the program is written in 

-  the efficiency of the compiler used 

-  the size of the input 

-  the organization of the input 

We frequently don't have control over the hardware a program will be run on – and you can imagine how annoyed users get when developers tell them “just use a faster machine ”! But we can control the language and the compiler, as well as the structure of the input. Common sense tells us that smaller input files that are arranged in a logical fashion can be processed more readily than large files with a random order of elements. Even when working with big datasets we can break those into smaller, more manageable pieces. 

One way to look at running time is to look at the number of times the instructions in the algorithm are executed. Here are 8 lines of pseudocode to illustrate: 

1. n = read input from user 

2. sum = 0
3. i = 0
4. while i < n 

5. number = read input from user 6. sum = sum + number
7. i = i + 1
8. mean = sum / n 

If we run this for a test dataset with n datapoints we might see: 

Statement 		Number of times executed 

1 

1

1

N+1

N

N

N

1

A simple example algorithm might be one that looks for the largest number in a series. For example: 

Given an array with these values: (1,2,3,4,5) 

We could write an algorithm that looks takes the first value in the array and puts it in a temporary variable – called it tempmax, say - then look at the next element in the array and compares it to tempmax. If the next element is larger, replace the value in tempmax with that element and repeat until you've looked at all of the values. In pseudocode: 

Tempmax = <first value in array>
     for i=2 to n (where n is the number of values in the array) 

nextval = (get next value in array)
if tempmax < nextval then tempmax = nextval 

This is a very simple algorithm, and not one we could optimize in terms of the organization of the dataset – it simply has to look at all of the values. 

Here's another example – a linear search algorithm that searches a dataset looking for a particular element: 

i=1
while (i<n and x <>a)
# NOTE: this means the loop runs n times as long as long as x isn't equal to a

 i=i+1 

if i <> n then location = i 

else location =0 

In this pseudocode we use the variable location to replace the actual search algorithm – this is just a demonstration, after all, and not runnable code! The test condition basically says that if you haven't found what you're looking for, set the value of the variable location to 0 and keep looking, but once you've found it, set the value of the variable location to i, which corresponds to the position where you found it! 

One of the lines in that pseudocode has a pound sign (#) in front of it – that is one of the ways developers can indicate a “comment” – code that isn't there to be executed, but there to remind the developer what that bit of the code is. Using comments is a GREAT practice – it makes it easier to pick up where you left off, or to go back and reuse code bits later. 

A linear search algorithm also can't really be sped up – it has to search until it finds what it's looking for. And if we were looking for a specific sequence element (e.g. any ATG in a DNA sequence), we probably would be interested in finding all of the matches, so we still have to process everything. 

So instead of a linear search, we could use what is called a binary search algorithm. A binary search algorithm iteratively narrows down the search field until it finds exactly what it is looking for. 

Let's take a look at a simple example. Let's say we are looking for the letter “j” in the entire alphabet: 

Right now the search “window” is the entire alphabet. But what if we just look at the first half of the alphabet: 

We can iteratively keep making the window smaller, say in halves until we actually find the letter “j”. I'm going to “skip” a few window sizes, you should get the idea: 

....almost there.... 

With this approach, we don't actually end up searching the entire array (or sequence) for what we're looking for – we iteratively narrow down the search. Here's how it would work in pseudocode: 

procedure binary_search(x: integer; a1, a2, ..., an: integers)


i = 1 {i is left endpoint of search interval – the alphabet in the example above} 

j = n {j is right endpoint of search interval} 

while (i < j) 

begin 

m = (i + j)/2
if x > am then i = m + 1 

else j = m 

end
if x = ai then location = i 

else location := 0 

What we're really doing here is just chunking the data set and looking for our value in small chunks rather than the whole thing. 

AGAIN – this only works if you are looking for one instance of something – if you wanted to search for all instances of ATG in a DNA sequence, you would still, obviously, have to search through the entire sequence! 

So the nature of the algorithm matters when it comes to optimization. But another important factor is going to be the size of the dataset. Small inputs aren't really a concern anymore – there just isn't very much complexity when you're searching 10 values. But when you are searching huge datasets of millions of values, the complexity becomes important! 

So let's look at data size as it affects optimization. Let's say that Bill and Ted have each written an algorithm to solve the same problem. Using the idea of running time from earlier, if Bill's algorithm has a running time or complexity of 5000n and Ted's algorithm is 1.1n (1.1 to the power of n), for a small dataset (10 values), Bill's algorithm takes 50,000 steps and Ted's takes 3 – clearly Ted's is better, right? But for a dataset of 1000 values, Bill's algorithm takes 5,000,000 steps and Ted's takes 2.5x1041 steps! 

So when developing your algorithms, it's important to thing about the nature of the dataset you'll be analyzing and what happens as the complexity changes. This way of thinking about the changes in time and space complexity as the data set increases is a very powerful way to compare algorithms. 

The idea of the growth of complexity of functions (not space) is usually described as “big-O” notation, and it refers to the “worst case scenario” of execution time for a program. Big- O analysis is used to measure the efficiency of an algorithm based on the time it takes for the algorithm to run as a function of the input size – but BigO itself is determined STRICTLY by the terms within the algorithm.  Another way to look at it is that Big-O is the complexity value of the algorithm! So when you see someone say “algorithm x is better because it is O(n) and algorithm y is O(2n)” it just means that algorithm y is twice as complex. 

We're going to go into some math theory here, but stay with me – the general concept is really the most important thing for you to get out of this right now, but you'll be expected to solve some simple examples of “big-O”. 

Let's look at types of complexity first, in order of least complex to most complex: 

Constants 

A constant value (e.g. “21”) is the simplest order of complexity, requiring no computation. 

Logarithmic Complexity or O(logn) 

More complex functions like our binary search (also called a bisection search) are logarithmic. A typical bisection search will find its results in 21 or so iterations regardless of the size of the dataset, but each iteration is more complex – so for small datasets it isn't the most efficient way to do the search, but it scales REALLY well. 

Linear Complexity or O(n) 

Basic addition and subtraction functions will be of a linear complexity, because the number of operations (e.g. if you are adding or subtracting) is directly proprortional to the number of digits. Linear complexity is a low level of complexity and should execute quickly. 

Quadratic Complexity or O(n2) 

Multiplication and division is more complex because you have more steps in the calculations – this is quadratic level complexity. 

Polynomial Complexity or O(nk) 

Here the complexity is a polynomial where k can be any constant – this sort of problem is solvable but very complex. Public Key Cryptography is a great example of this type of complexity, computing two prime factors of a large number. 

Problems that can be solved with polynomial worst-case complexity are considered tractable, or reasonable. Problems of a higher complexity are considered intractable, or unreasonable, and they may include: 

Exponential Complexity or O(Cn)


Factorial Complexity or O(n!)


As an aside - if there's no algorithmic solution to a problem at all, we call it unsolvable. 

So what if I have an equation that uses more than one type of mathematical function (e.g. both a linear and a quadratic type of function)? Ignore the simpler one and calculate the Big O using only the most complex type of function, since the lesser complexity stuff will be “trivial” in comparison. There's a chart later on that shows the order of complexity for common mathematical functions..... 

Let's look at an example using just math. Our function f(x) that we will test for complexity (the O) will be a quadratic formula. 

Say we want to show that f(x) = x2 + 2x + 1 is O(x2) (quadratic complexity) For x>1 we then construct:
x2 + 2x + 1 ≤ x2 + 2x2 + x2
This simplifies algebraically into: 

x2 + 2x + 1 ≤ 4x2
For C = 4 and k =1, we have: f(x) ≤ Cx2 whenever x > k So the function f(x) is O(x2)! 

It might make more sense if you see it as a table, where some basic mathematical functions are display along with how quickly their algorithmic complexity increases as the dataset of size n grows in size: 



--- SOURCE: Week 3 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week 3 Lecture – Advanced Bioinformatics Data Structures

Last week we discussed algorithms and algorithm complexity, or BigO.  This week, we’re going to look at bioinformatics data structures (file formats).  You should already be familiar with couple of file formats, but we’ll go over all of them as a refresher.

SIMPLE FILE FORMATS

FASTA:

Recall that a FASTA file (.fasta or .fa) is the simplest way to store sequence data.  A FASTA file can store one or more sequences, each using two lines – one for the “header”, which contains a special character (>) to denote that it is the header, followed immediately by the sequence:

>XR_002086427.1 Candida albicans SC5314 uncharacterized ncRNA (SCR1), ncRNA

TGGCTGTGATGGCTTTTAGCGGAAGCGCGCTGTTCGCGTACCTGCTGTTTGTTGAAAATTTAAGAGCAAAGTGTCCGGCTCGATCCCTGCGAATTGAATTCTGAACGCTAGAGTAATCAGTGTCTTTCAAGTTCTGGTAATGTTTAGCATAACCACTGGAGGGAAGCAATTCAGCACAGTAATGCTAATCGTGGTGGAGGCGAATCCGGATGGCACCTTGTTTGTTGATAAATAGTGCGGTATCTAGTGTTGCAACTCTATTTTT

GENBANK:

Much richer in content is a Genbank File (.gb).  A Genbank file has a simple structure that allows it to be parsed; it contains a sequence plus descriptive data (e.g. features, sources, references) for that sequence:

Sample Genbank Record file at NCBI

TABLES:

One common way of storing data in many sciences is in table format.  In bioinformatics, tables are typically plain text files (not Excel) that are stored in .tsv (tab-separated) or .csv (comma-separated) format.  The delimiter (the tab or comma) makes these tables easy to parse, since there is a unique character separating each field:

CSV:

250010605000007,8,67,31,98,68.367346938775510,HUMAN

250010623400007,8,67,31,98,68.367346938394510,HUMAN

523352425525252,3,61,31,98,68.367342948775510,CHIMP

250042353000007,8,67,31,98,68.367346938749510,HUMAN

232662345252423,5,61,31,98,68.364956938775510,CHIMP

	TSV:

250010605000007	8	67	31	98	68.367346938775510	HUMAN

250010623400007	8	67	31	98	68.367346938394510	HUMAN

523352425525252	3	61	31	98	68.367342948775510	CHIMP

250042353000007	8	67	31	98	68.367346938749510	HUMAN

232662345252423	5	61	31	98	68.364956938775510	CHIMP

You probably noticed that the tab-separated table is much easier to read – legibility, however, is frequently less important than extractability.  Using a comma as a delimiter can definitively separate fields; tabs may be more prone to error during algorithmic parsing, particularly if there are blank fields.

ADVANCED FILE FORMATS

FASTQ:

A .fastq file (.fastq or .fq) is an enhanced form of FASTA file.  While a FASTA file contains two lines per sequence, a FASTQ file contains four – a header (which is indicated by an “@” instead of the “>” used in FASTA files), the sequence, a “+”, and then the quality string:

@K00188:208:HFLNGBBXX:3:1101:1428:1508 2:N:0:CTTGTA

ATAATAGGATCCCTTTTCCTGGAGCTGCCTTTAGGTAATGTAGTATCTNATNGACTGNCNCCANANGGCTAAAGT

+

AAAFFJJJJJJJJJJJJJJJJJFJJFJJJJJFJJJJJJJJJJJJJJJJ#FJ#JJJJF#F#FJJ#F#JJJFJJJJJ

The quality string represents the sequence quality at each base position, and represents the PHRED quality score as determined by the base calling software used to generate the data.

SAM:

A .sam file is a sequence alignment map.  It is the result of taking sequence data (e.g. shotgun sequence reads) and aligning all of the reads against a reference sequence (e.g. the human genome).  Thus, the SAM file contains all of the positional information for the sequencing reads aligned to the reference.  It’s not at all unusual for sequencing reads (in particular the older “next gen” reads, which are shorter than the more modern “long read” technologies) to align to more than one position, and the SAM file will show all of the matches.

A SAM file is far harder to read by eye – it is really meant as a way to store the alignment data for further programmatic manipulation:

1:497:R:-272+13M17D24M	113	1	497	37	37M	15	100338662	0	CGGGTCTGACCTGAGGAGAACTGTGCTCCGCCTTCAG	0;==-==9;>>>>>=>>>>>>>>>>>=>>>>>>>>>>	XT:A:U	NM:i:0	SM:i:37	AM:i:0	X0:i:1	X1:i:0	XM:i:0	XO:i:0	XG:i:0	MD:Z:37

19:20389:F:275+18M2D19M	99	1	17644	0	37M	=	17919	314	TATGACTGCTAATAATACCTACACATGTTAGAACCAT	>>>>>>>>>>>>>>>>>>>><<>>><<>>4::>>:<9	RG:Z:UM0098:1	XT:A:R	NM:i:0	SM:i:0	AM:i:0	X0:i:4	X1:i:0	XM:i:0	XO:i:0	XG:i:0	MD:Z:37

19:20389:F:275+18M2D19M	147	1	17919	0	18M2D19M	=	17644	-314	GTAGTACCAACTGTAAGTCCTTATCTTCATACTTTGT	;44999;499<8<8<<<8<<><<<<><7<;<<<>><<	XT:A:R	NM:i:2	SM:i:0	AM:i:0	X0:i:4	X1:i:0	XM:i:0	XO:i:1	XG:i:2	MD:Z:18^CA19

9:21597+10M2I25M:R:-209	83	1	21678	0	8M2I27M	=	21469	-244	CACCACATCACATATACCAAGCCTGGCTGTGTCTTCT	<;9<<5><<<<><<<>><<><>><9>><>>>9>>><>	XT:A:R	NM:i:2	SM:i:0	AM:i:0	X0:i:5	X1:i:0	XM:i:0	XO:i:1	XG:i:2	MD:Z:35

In the example above there are 4 entries which have text-wrapped to fit into this file.  SAM files use a TAB-delimited text format with header and a body; header lines start with ‘@’ and contain descriptive information (much as the comment lines beginning with ‘#’ do in a python program).  The alignment records make up the rest of the file. Each alignment record has at minimum 11 mandatory fields describing the alignment; additional fields can be added depending on the software being used:


source: http://samtools.github.io/hts-specs/SAMv1.pdf

BAM:

A .bam file is the compressed binary form of a SAM file.  As you can imagine, a SAM file containing all of the reads generated by an automated sequencer aligned against a reference such as the human genome can create a fairly large file (one of my “simpler” SAM files is 787 megabytes in size).  By compressing the reads to a format that is only readable by software, you can reduce the file size significantly (the BAM file for my 787 Mb SAM is only 131 MB), making a BAM file an ideal “storage” format for alignment data.

BAI:

A .bai file is an index file that stores lookup information for a BAM file.  Typically you’ll sort the BAM file first (more on that later), then index it – and the BAI will have the same name as the BAM (other than the extension, obviously) for easier identification.  You don’t have to index your BAM files for every application, but some programs (the Integrated Genomic Viewer, or IGV, for example) absolutely require an index to accompany each BAM.

BED:

A .bed file is a Browser Extensible Data file that stores feature annotation information by coordinate relative to a reference.  This format was originally developed during The Human Genome Project as a way to add custom annotations to web-based genome views, and is widely used because of its coordinate-based (rather than sequence-based) format, which makes it far easier to parse and process.

BED files are actually a very structured form of table (see .csv/.tsv above):

chr7    127471196    127472363

chr7    127472363    127473530

chr7    127473530    127474697

In this example there are no headers, and only the 3 required columns (chromosome, start position, stop position), separated by tabs.  In practice most BED files have multiple additional columns for information such as coding strand, name, score, and values to describe the visual elements of the annotation in a viewer, such as color.  For more details see: https://m.ensembl.org/info/website/upload/bed.html

GTF/GFF:

Both .gtf and .gff file formats also contain annotation information, but are far more structured in their format than BED files.  They are very similar in format, differing primarily in the ontology and naming conventions used to name annotations, and are always stored as .tsv tables:

The columns, unlike BED files, are well defined – you can see the full details here:  https://seqan.readthedocs.io/en/master/Tutorial/InputOutput/GffAndGtfIO.html

VCF:

Variant Calling Format files, or .vcf files, are also tabular files that store (as you might have guessed) variant information that was called by software that looks at alignments (like a BAM file) and determines whether or not the experimental data from the sequencers (which, as you recall, is aligned to a reference, frequently a genome, in the BAM file) differs from what is expected.  For example, if you are looking for SNPs or variants or isoforms in a patient’s DNA, you would sequence their exome (express-ome, or the mRNA from genes that are expressed), align the resulting output sequence data to the human reference genome to create a BAM file, and then you would run that BAM file a variant caller (more on this later).  

VCF files are highly structured and defined.  They always container metadata in headers (starting with “##”) and have 8 mandatory columns (#CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO).  You can see an example here:  

https://samtools.github.io/hts-specs/VCFv4.2.pdf

SQL:

As I promised, lots of bioinformatics data is stored in table formats!  There’s one more data structure we need to discuss, which also uses tables, but not easily accessible tables like the above examples, and that is SQL.  SQL stands for Structured Query Language, and it is a way of constructing and interacting with one or more tables that are contained within a relational database.  SQL databases are very powerful, capable of storing large quantities of data and sorting/retrieving individual records very quickly – they are used almost everywhere in business (think financial transaction data at your bank), science, and yes, bioinformatics.

There are both commercial and free SQL-based databases out there, and since SQL itself is a standard language you can write SQL statements that will work in Oracle™, Microsoft™, or free SQL databases like MySQL, PostgreSQL, and MariaDB.  We’ll go into more detail on SQL databases next week – for now, if you want more information, check out MariaDB, which we’ll be working with next week:  https://mariadb.com/kb/en/introduction-to-relational-databases/

WORKING WITH BIOINFORMATICS DATA STRUCTURES

Now that you have a broader idea of the types of data structures we use in bioinformatics, let’s walk through some of the ways you will be working with those data structures.  In this example, you are a researcher at a university that has been sent a DNA sample from a local hospital.  They want you to see if there are any genetic anomalies in the patient’s genome that may explain their symptoms.  Your analysis includes the following steps:

Sequence the DNA using your next-next-gen sequencer (we’ll assume that all of the wet-lab work required for this has already been done by someone else in the lab; this is bioinformatics, after all!).

Input:  DNA

Output:  one 5Gb FASTQ file containing all of the shotgun reads for the patient’s DNA

You want to generate some quick statistics on the data (e.g. the average read length), and to simplify you want to convert the FASTQ to a FASTA file.  There are many ways you can do this, including using AWK or SED from the BASH command line.  Since we aren’t going to talk about linux until the end of the term, though, let’s use a simple online conversion tool like seqret, which is available through EMBOSS.  If you have EMBOSS installed on your local machine you would just open a command-line prompt (the terminal on a Mac or linux machine) and type:

	seqret -sequence reads.fastq -outseq reads.fasta

Alternately you can use a web-based version of the tool at any EMBOSS server (you can find them using Google).   Another nice alternative is SeqTK, but for a more comprehensive list you can check out this link:  

https://bioinformaticsworkbook.org/dataWrangling/fastaq-manipulations/converting-fastq-format-to-fasta.html

The next thing you’d like to do is to create a BAM file.  Recall that a BAM files consists of your sequence data aligned to a reference genome.  So you need to find a good mapping algorithm that will map your FASTQ to the latest version of the human genome, GRCh38.  You install minimap2 from github (arguably the best aligner for next-next-gen sequence data) and GRCh38 and you are ready to go.  From the command line on your computer you invoke minimap2 to map your FASTQ to GRCh38:

./minimap2 -a GRCH38.fasta my_sequence_data.fastq > my_data.sam

INPUTS:  human genome FASTA, sequence data FASTQ

OUTPUT:  SAM file

Now you have a SAM file that contains your sequence data mapped to the human genome.  Unfortunately, most software packages want to have a BAM file as their input, so to do anything else you will have to convert your SAM to a BAM.  Luckily you know that the software package samtools can do exactly what you need (and more).  So you install samtools and install it.  Once it’s installed you simply type this line:

samtools view -S -b my_data.sam > my_data.bam

What we are doing here is viewing the SAM file (-S means input is a SAM) and outputting a BAM (the -b).  Since that simply views the file in the terminal, we use the “>” symbol to redirect the output to the file my_data.bam.  

We aren’t quite done, however – we’ll need to sort and index the BAM file so that we can use it later.  Luckily samtools can do that too:

samtools sort my_data.bam -o my_data.sorted.bam

samtools index my_data.sorted.bam

INPUT:  SAM file

OUTPUTS:  BAM file, sorted BAM file, BAI file (BAM index)

Samtools is really pretty simple to use  – you can read more at http://quinlanlab.org/tutorials/samtools/samtools.html

Now that we have our sorted BAM file and index file, we can do the actual work of finding any variation.  To do that, we need to run a variant caller to detect any variants.  There are many different variant callers, and in practice it’s often best to run several and compare their output.   One of the first most popular variant callers, part of the samtools software suite, is bcftools mpileup.  One of my personal favorites is FreeBayes, a more modern variant caller that (as you may have guessed) uses a Bayesian approach.  For purposes of this example, let’s keep it simple and use bcftools mpileup:

bcftools mpileup -f GRCH38.fasta my_data.sorted.bam | bcftools call -mv -Ov -o calls.vcf

Notice that we feed mpileup the reference as well using the “-f” command– that’s because the BAM file, although containing sequences with positional information relative to the reference, doesn’t actually contain the reference itself.  This keeps the file sizes smaller.

If you are familiar with linux, you may also have noticed that we are actually running two commands on one line here, using the “|” symbol between them.  The “|” indicates that we want to pipe the output of the first command to be the input to the second command.  This is an extremely useful command to know, and you’ll probably use it frequently!  The first line generates genotype frequencies at each position, and the second makes the actual calls and generates the VCF file that contains all of the information on variants – their positions and frequencies.  If you want to know more about what all of the various flags and switches do you can read more at https://samtools.github.io/bcftools/howtos/variant-calling.html.  

INPUTS:  GRCH38.fasta, my_data.sorted.bam

OUTPUTS:  calls.vcf

Now that you have the variant calls, the hard part begins – interpreting the results.  Your variant caller will find all sorts of variants, and some of them will be false positives.  You will undoubtedly also have missed some variants due to the nature of the data and the variant caller parameter settings (which is why it’s good to run many different callers and compare results).  More importantly, however, is the fact that every human will have variants from the reference genome – the trick now is to determine which of the variants in your data is responsible for the disease.  While the intricacies of this are beyond the scope of this class, you can expect that you will have to sort and filter your VCF file in a variety of ways before you find the answer, and that will require more programming on your part.  We will talk a little bit about some of the tools you can use for sorting and filtering tables in week 11.

In the example above we used genomic DNA and did shotgun sequencing to detect genomic variants.  There is another form of variation that would not be detected by this method, and that is Isoform detection.  Isoforms are splice variants, some of which are canonical, others of which may be the result of erroneous splicing, recombination, or other genetic anomalies.  To detect isoforms you would start with exome sequencing and subject the results to a slightly different pipeline (instead of variant calling you would use an isoform/fusion caller like SQANTI).  In general, however, the tools used in the pipeline above should show you what a good algorithm pipeline can do, and how the different bioinformatics data structures are involved – and can be manipulated.

In closing, if you are thinking to yourself “Gosh, so much of this seems to run (or run better) from a linux command line”, you are correct!  Not only that, but linux and all of the apps mentioned above are available to anyone, at no cost, and they are typically well-supported by the community (meaning updates and bug fixes happen pretty regularly).  You can run also them on MacOS (which is really linux under the hood), or on Windows – but I strongly recommend that you consider getting some linux experience.  Start by installing linux on an old laptop or desktop you have laying around (it can run on surprisingly low-end hardware), or if you feel adventurous set up your computer to dual-boot your main OS as well as linux – it will take time, but if you are serious about bioinformatics, it will be worth your time!



--- SOURCE: Week 4 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week 4 Lecture – Indexing & Searching, SQL databases

Genbank is without a doubt one of the world’s largest repositories of sequence data.  You can find sequences there in a variety of ways – for example, if you know the accession number (a unique identifier for each submitted piece of sequence data), you can type that into a search bar and almost immediately retrieve the data.  If you are going in the opposite direction – let’s say you have some sequence data but don’t know what the sequence is – you can put your sequence into the BLAST search window and search the entire database by sequence and find your sequence (and the accession number) that way.  Searching by BLAST takes considerably longer than searching by accession number, but both have been heavily optimized to make finding what you are looking for as quick as possible.

Part of optimizing these searches involves indexing.  In the first example, where you are searching by accession number, the process is very straight-forward:  your query (the accession number) is looked up in a table that contains the accession numbers in one column and a document ID in another.  The accession numbers are sorted alphabetically, which makes it trivial to quickly do the lookup and match the accession number to the document.

HASH TABLES

Searching by sequence, however, is much more challenging.  BLAST does it’s initial matching by searching for K-mers – a sequence word of a given size, say 3 (for protein searching) or 11 (for nucleotide searching) and then extending all the matches in both directions, scoring the alignments and only returning the high-scoring matches (HSPs).  For every K-mer of a given size, there are millions of sequences that match that particular string – think of how many ATGs occur in Genbank, for example (if you don’t know, ATG is the universal START codon, so it’s at the beginning of every gene, and at every other methionine-coding triplet)!  As of 2019 Genbank contained 3.6 trillion bases – searching each sequence entry individually would take a ridiculous amount of time.

So how does Genbank return results so quickly?  It’s not just the number of servers they have – it’s the way the data is stored.  Genbank indexes sequences (not just records, the actual sequences themselves) by creating hash tables.  Let’s walk through an example of what these are, and how we can use them.

Within the sequence below, for example, TTAC is a 4-mer that can be found twice, at positions 3 and 17: 

GCTTACAGATTCAGTCTTACAGATGGT 

Obviously searching a short sequence like this is easy.  What we need to do is find all instances of our K-mer (TTAC, in this case) in the entire database – that’s where hashing comes in.  Hashing takes the list of all of the possible K-mers for a given size (e.g. 4 or 11) and then stores the positions where each of those can be found within all sequences – generating a hash table (sometimes called a lookup table).  To make things faster (and more compact), BLAST uses an integer to refer to a given K-mer by its first residue and then stores the positions relative to the integer.  The diagram below displays a hash table scheme for a few protein sequence 10-mers:

Hashing provides us with a lookup table, or hash table, that tells us where we can find specific sequence words, which greatly speeds up our searching process – effectively serving as an index for sequence data.

For more information on hashing as it is applied to bioinformatics data, please see:

https://www.cs.rice.edu/~ogilvie/comp571/2018/09/04/blast-fasta.html

K-mers are important for a variety of reasons in bioinformatics – we’ll go into more depth on them in Week 9.

KEYWORD TREES

Even with Hash Tables searching for exact patterns can take a significant amount of time.  Fortunately, there are other approaches we can use to speed up searching.  One common one is to use a keyword tree. Keyword trees essentially make a chain where branches occur at each point where the items being indexed have differences. Let's say we want to build a keyword tree for two words, Apple and Application. The first four letters are identical, but the words start differentiating at the fifth letter, so our keyword tree would look like the image on the left, below.

Notice how the common elements are shared in the tree (the first four letters:  A, P, P, L).  After the letter “L” we have a branching point in the tree, since that’s where the words start to differentiate from one another.

The root doesn’t have to have just a single starting point – that would make the tree useless for any word that didn’t start with “A”!  Words starting with a different letter (say BALE) would have their own branch in this tree, as in the image on the right, below:

‘

As you can imagine, you can extend this infinitely.  Luckily in bioinformatics there are only 4 DNA bases and 20 (ish) amino acids, so keyword trees for bioinformatics are manageable.  Keyword trees store a set of keywords in a rooted labeled tree, where each edge (connecting line) is labeled with a letter from the alphabet. Two edges (lines) coming from the same vertex, or node, will have different labels (e.g. A or B). The important thing is that every keyword stored in this tree can be spelled on a specific, individual path from the root to an end-point, or "leaf". So to get to a specific word (e.g. when searching), you just follow the path from the root, making the appropriate decisions where applicable (e.g. is it an E or an I). Once you reach an end leaf, your search is done. 

SUFFIX TREES

Another way to index information is to use a suffix tree.  A Suffix tree is similar to a keyword tree, except that the common edges in a path are collapsed: 

Suffix trees have the advantage that they can be built far more quickly and easily than keyword trees – a keyword tree requires quadratic complexity, while a suffix tree requires only linear complexity.  The disadvantage is that suffix trees can take significantly more space to store.

To search a keyword (or suffix) tree, you "thread" your way through the text of the tree, comparing the text your are searching with the options given in the tree. Threading is considered "complete" when you reach a "leaf" (endpoint) in the tree, at which point the pattern search is complete. 

This works great for exact pattern matching – but since biology is a "wet" (some some "squishy") science, there is considerable diversity and divergence caused by mutations. As such, exact searching isn't always what we are looking for – often times a heuristic approach is more appropriate, allowing us to find approximate matches. This is critical when you are looking for genes that are similar to your "gene of interest" – perhaps you are looking for genes with similar yet different functions, or homologous genes in different organisms. 

RELATIONAL (SQL) DATABASES

Large amounts of tabular data are typically stored in a relational database, commonly on a server.  If that database uses the standard SQL, or Structured Query Language, then it is a SQL server.  There are many different SQL server packages around, including Oracle and Microsoft’s commercial versions, but most bioinformaticists (unless they work for a big pharmaceutical company) will use one of the freeware variants.  MySQL, MariaDB, and PostgreSQL are all excellent SQL database tools that you can use at no cost, and since they are all based on the SQL language, once you know how to work with one of them, you will (with minor differences) be able to work with any of them.

USE CASES

What, exactly, are relational databases, and why would we use one in bioinformatics?  According to the Oracle web site:

A relational database is a type of database that stores and provides access to data points that are related to one another. Relational databases are based on the relational model, an intuitive, straightforward way of representing data in tables. In a relational database, each row in the table is a record with a unique ID called the key. The columns of the table hold attributes of the data, and each record usually has a value for each attribute, making it easy to establish the relationships among data points.

(https://www.oracle.com/in/database/what-is-a-relational-database/)

Relational databases keep the logical data structure separate from physical storage – for example, if you rename a database file, the tables stored within it don’t change.  The relationship between the tables inside the database, if you will, is the critical part – rather than store everything in one huge spreadsheet, you break the data logical tables of data.  You use keys and indexes within tables – and between tables - to make it easy and fast to search and sort the data.  Relational database ideal when your data matches several or more of the following criteria:

Multiple simultaneous changes to data (concurrency) 

Data changes on a regular basis 

Large data sets where you only need some observations/variables 

Share huge data set among many people 

Rapid queries with no analysis 

Web interfaces to data, especially dynamic data 

Bioinformatics data matches all of the criteria above.  For example, the UCSC Genome Browser contains a vast wealth of information beyond the “simple” sequence data of the human genome.  There are tables with containing information about genes, annotations on those genes, annotations on the actual sequence data (e.g. CpG islands), homologs, literature references – everything you see when you browse the genome is contained in these tables.  Separate tables are used for specific information – this makes the information easier to search and index.

Take a look at this web site – it lists all of the individual tables used for the UCSC genome:

https://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/

You can imagine how much of a data disaster this would be if you had all of this data in a single table (e.g. an Excel spreadsheet)!

If you poke around UCSC’s site you will see that they use a successor to MySQL (one of the older, more robust open-source SQL databases) called MariaDB.  MariaDB is one of the friendlier, yet more powerful, packages, and it’s what you’ll be using for one of your homework assignment this term.



--- SOURCE: Week 5 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week 5 Lecture – Heuristic Searching

Searching a database is typically a straightforward affair – you enter a query and you expect to get back the direct answer to your results.  When the size of the data that you are searching within becomes increasingly large, however, the speed of doing a complete, comprehensive search can become equally large.  In fact, there are problems that we consider unsolvable because of the lack of an efficient algorithm that can solve them.  One example that you may already be familiar with is that of maximum parsimony.  Let’s walk through an example:

For a given number of species (taxa), there is a “true” phylogenetic tree that graphically describes how they evolved from a hypothetical ancestral state (root), and the branches leading to the taxa will have various nodes that represent previously-extant common ancestors for that taxa branching off of them.  The problem is that there are a large number of possible branching patterns (trees) for any number of taxa, and only one is the actual correct one.  The idea behind Maximum Parsimony is that you build all possible trees, map the informative characters onto those trees, and then select the one tree that has the fewest number of evolutionary changes (the most parsimonious tree).  A simple example:  let’s say you have 4 taxa and you want to create a maximum parsimony tree.  To compute the number of possible rooted trees, you can use this formula:

(2n-3)!/[(2n-2)*(n-2)!]

(see http://carrot.mcb.uconn.edu/mcb396_41/tree_number.html)

Plugging 4 in for N gives us 15.  So for 4 taxa it’s relatively easy to construct and evaluate all trees.  Very few papers, however, are concerned with the phylogeny of 4 taxa.  So how many possible trees are there for an interesting number of taxa – say, 30 (which is how many my first publication dealt with)?  Rather than doing the math you can just look the answer up on the link above to see that the number is 8.69E+36.  For an even larger number of taxa, say 60, the number is 5.01E+94 – that’s more possible permutations than the number of protons in the universe!

Clearly we can’t realistically search the entire possible solution space for a large number of taxa – not in any reasonable amount of time.  We call this an NP-Complete problem – nondeterministic polynomial, because it takes polynomial time to solve.  So how do we solve this kind of problem?

The answer is that we don’t search the entire solution space.  We take a short cut – a heuristic approach.

HEURISTICS

A heuristic is defined as a problem-solving method that uses shortcuts to arrive at a “good enough” solution in a limited amount of time.  So a heuristic algorithm, then, produces an acceptable solution to a problem, albeit without any ability to prove that it is in fact correct.  Given that the alternative is to wait until the heat-death of the universe and probably still not have an answer, heuristic approaches are, well, acceptable.

Heuristic approaches constrain the solution (or search) space to help minimize false solutions.  While they promise speed and accuracy, you typically have to sacrifice one or the other, at least partially, and they work better with larger sample sizes and well-defined problems - problems like:

Sequence alignment

Sequence searching (including BLAST)

Parsimony-based phylogenetics

Motif detection

Protein docking

Protein structure resolution

Let’s take a look at a few bioinformatics examples where heuristic approaches are used:  

FASTA

FASTA is far more than a file format – in fact, the file format was designed to be used with the FASTA sequence search algorithm.  The FASTA algorithm predates BLAST, and while not used as frequently anymore, it still has value – while slower than BLAST, it is far more sensitive for nucleotide searching and works better than BLAST for repetitive sequences.  In addition it works very well for gapped local alignments.

Similar to BLAST, the FASTA algorithm uses a word-length starting point, which is called a ktup, and applies penalties for both opening and extending gaps.  Scoring is also similar, using an E (expectation) score, but with the addition of a Z score.

Here’s how FASTA works:

Step one:

Find exact matches of word size between query and target, record in a hash/lookup table – the hash/lookup can be pre-computed for different searches k=1 (oligo 20nt), k=6 (normal 100-500nt)

Step two:

Cluster the ’hot-spots’ into diagonals by making a matrix of 1s and 0s by position

score all of the diagonals with each region (+) and each gap -

find the 10 best diagonals and then perform a local alignment with no indels

the best partial alignment is called init1 and is used in Step Four

Step three:

going back to the 10 partial alignments, the algorithm now takes any that exceed a certain score cut-off and tries to make them into longer alignment runs

if a longer partial can be made (and this is a graph theoretic problem) it is optimally aligned and returned as one result from the algorithm 

Step four:

picking up the init1 partial alignment from Step Two the algorithm performs a banded Smith Waterman

a window of alignment space either side of the init1 diagonal is identified and optimal local alignments are performed throughout the space

the alignments are scored by matrix and statistics are calculated, normalised Z-score and E value

As an output, FASTA returns both a scoring histogram showing the distribution of matches as well as tabular output of the best matches (much like BLAST):

Histogram:

Matches:

FASTA is heuristic in that it initially finds a series of word hits – the word-to-word matches of length k – and identifies potentially “best” matches before performing a more time-consuming optimized search using a Smith–Waterman type of algorithm.  This doesn’t mean that it didn’t miss a better match somewhere else; by using the word of length k it might have missed a better match that it might have found had it used a different word, even with the same length of k.  We call this the problem of local optima – the heuristic approach found a good match given it’s starting point, but it might have missed a better solution that exists elsewhere in the solution space.

The same basic problem (that of local optima) exists with all heuristic approaches, including the next example – maximum parsimony.  While you’ve been exposed to parsimony before, in this discussion we are interested primarily in how it uses heuristic approaches – shortcuts – to find an answer that is “good enough” for a problem where we cannot search the entire solution space.

MAXIMUM PARSIMONY

Recall from above that with parsimony the number of possible trees (the search space) increases exponentially with the number of taxa.  Thus, maximum parsimony methods use heuristic approaches to find an answer that is “good enough”.  How does this work?

One approach is to construct a starting tree by adding one taxa (sequence) at a time until there is a complete tree, scoring that tree, and then performing branch swapping – rearranging nodes to see if the score can be improved.  In some approaches, branch swapping occurs during sequence addition.  For example, until there are more than a given number of branches (say, 9), the algorithm can try all possible combinations (there aren’t that many) and finds the optimal tree – below is a purely hypothetical construct of a tree with 9 taxa:

Now the algorithm adds another sequence to this “best” tree.  Rather than trying all possible combinations (and again the number when this happens doesn’t have to be 9, it could be 10 or 12 – wherever the algorithm is designed to start doing heuristics instead of exhaustive searching), the next sequence is added to an existing clade (e.g. the group with taxa 3, 4, and 5) and the tree evaluated.  Next the new branch is swapped to another clade (e.g. the group with 7, 8, and 9) and the tree evaluated.  This is repeated until the new taxa has been tried in each of the clades, and the best scoring tree is retained and the process repeats with the addition of yet another taxa. 

NOTE:  Some algorithms also perform branch swapping at the end of the tree construction process – swapping the terminal nodes within each clade to see if the score can be improved.  There are several variations on when and where branches are swapped, but the general idea is to not evaluate all trees and just make a “best guess”.

So you can see how this approach could lead to a local optima – what if a taxa is actually better placed somewhere else, but the swapping algorithm didn’t try placing it there because it was “too far away” from the starting node?

There are ways of compensating for this as well.  Since this problem is caused by adding a taxa/sequences later in the process (after full evaluation is possible), one way to compensate is to generate multiple trees from the same data set, jumbling (randomizing) the input order of sequences each time so that the sequences are not always added in the same order.  Since the trees will most likely be slightly different with each run, you then generate a consensus tree by computing the number of times each node appears and displaying that at the node.  

Jumbling allows you to minimize the chances that a sequence is placed in a less-than-optimal location simply because of when it was added relative to the other sequences.  Ideally you will jumble (randomize the input order) and create a tree 10 (or more) times, meaning that you are actually generating 10 (or more) “optimal” trees.  Of course, this means that the algorithm runs 10 times longer, but with the benefit of probably improved accuracy – and it still doesn’t take nearly as long as evaluating all possible trees!

NOTE:  a technique called bootstrapping is also often used when constructing phylogenetic trees.  It is used to ensure that there is no positional sensitive to tree construction (e.g. that there aren’t columns in the alignment that skew the construction one way or the other) by resampling the data and constructing multiple synthetic datasets from the original dataset.  This is typically done 100x (generating 100 synthetic datasets) and again a consensus tree made.  So if you combine bootstrapping and jumbling with the recommended minimum numbers (100 bootstraps, 10 jumbles) your algorithm now runs 1000 times – and it is still faster than evaluating all possible trees!

Hueristic approaches can be applied to any problem where the possible solution space is so large as to make an exhaustive search computationally intractable.  Bioinformatics, as you know, has many such problems.  Be sure to read this week’s OERs for some more elaborate examples!



--- SOURCE: Week 6 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week 6 Lecture - Troubleshooting Code & Data

During Midterm week we are going to change things up a bit and talk about something that’s actually very hard to talk about – troubleshooting.  The reason it’s hard to talk about is that there’s no magic formula or method to apply other than persistence – and paying attention to all of the information you have.  It’s something you really learn by doing, rather than by reading about it – but since you will need these skills later on in this class (and pretty much for the rest of your life), I’m going to take a stab at helping you figure it out.

We can break this topic down into several categories, each of which will have its own delights:  hardware, code, and data.  For purposes of this class, you’ll have to troubleshoot code and data, but it’s easier to begin with something that you all have experience with – hardware.

Troubleshooting Hardware

Let’s start with a ridiculously easy example – lightbulbs.  You’ve all had this happen:  a lightbulb doesn’t come on when you flip the light switch.  There are many things that could be wrong – maybe the power is out, maybe the switch is broken, maybe the wiring is bad – maybe it’s even the bulb.  Subconsciously we do a lot of troubleshooting in our brains:  if the power is out, then all the lights will be out, so that’s easy to rule out.  We almost always flip the switch several times, but that just confirms that something is broken.  The next easiest thing to test is actually replacing the bulb – and if we do that and the light comes back on, then we’re done.  But what if it doesn’t?  We have to look deeper.  Is the breaker thrown for that circuit?  We can easily check the breaker box and see.  If power is indeed on to the circuit, and the light doesn’t come on, and we’ve already changed the bulb, that leaves the wiring or the switch.

Most of us stop there, but an electrician will next check if there is really power coming to the switch by using a multimeter.  If there is power to the room (breaker box), a working bulb (because we checked and maybe even replaced it), and power to the switch, then we know it’s the switch.  Simple, right?

Troubleshooting is almost always a matter of first figuring out what could be wrong, then eliminating each item one at a time – usually in the order of “most obvious”, but sometimes also by looking at “easiest to check”.  Let’s walk through another example – one that we have also all experienced:  a computer that doesn’t boot up.  We’re going to use a desktop computer as an example since it’s something that is far easier to fix/troubleshoot than a laptop.

Let’s imagine you are sitting at your desk and you push the power button on your computer and nothing happens.  What are the first things we check?  What are the obvious problems that might be causing the computer not to turn on?

Not plugged in

Not powered on at power supply

Monitor not plugged in

These are the easiest ones to check – you can literally just look at the computer and the outlet and see if it’s plugged in.  You can also check the switch at the back of the power supply and make sure it’s still in the ON position.  Assuming that those all check out, what would be our next steps?

When you push the button, does the monitor come on and go to the BIOS screen?

If this happens you know that the power situation is fine and in fact the motherboard/CPU are fine.  This leaves you with something wrong with the software – and we’ll skip that for now…let’s assume the monitor doesn’t light up:

Are any of the lights inside the computer on?

There are usually little indicator lights on the motherboard that will light up during the POST process (Power On Start Tests).  If these lights come on, then you know that the motherboard is getting power – so at least it’s not your power supply.

 Do you need to replace the BIOS battery?

The only way to check really is to replace it, unless you have a voltmeter.  But often computers won’t start up if this battery is really old/dead.  If this doesn’t do it, then:

Something is very badly wrong with the motherboard, CPU, or RAM.

There are steps to take to check this as well – but we’re getting a bit beyond what we need to for purposes of this lecture.  And while what we’ve gone over is by no means an extensive list, these are some of the simple steps you can take to help keep the Geek Squad™ out of your house as long as possible.

Let’s move on to something that you are more likely to run into during the course of this class – troubleshooting code by creating and troubleshooting a simple R script.

Troubleshooting Code

One of the reasons we use modern Integrated Development Environments (IDE’s) is because they do a lot of things to help us write better code – things like showing us the current values and types of variables (which is far more useful than it sounds – if you need your data to be cast as an integer and the software read it in as characters, well, know what the problem is puts you halfway towards solving it), autocomplete to help avoid typos, and debugging that shows where something is just plain broken (like a missing library, bad syntax, etc).  In this lecture we’re going to focus on R code in RStudio, but any good IDE will help you with these problems – e.g. PyCHARM or Spyder for Python, or Intellij’s IDEA for java.

Let’s start by opening up RStudio.  Create a new R script from the FILE menu:

I always like to start my scripts with some basic information in comment lines – you can change these to suit you:

# R Script for Sequence Download & Comparison

# Dr. R. Wolfgang Rumpf

# November 2022

# This script gives you a quick and easy introduction to some simple sequence

# tools available for use in R.  This is by no means a comprehensive list!

Since this is comment code, we don’t have to worry about running it!  Now let’s make sure that we have installed the packages that we know we’ll be using for this exercise – we’ll need a few – seqinr, ape, and the msa library from Bioconductor.  The first two are installed using the standard technique for R:

# First let's install any packages we need if we don't already have them

if (!require("seqinr")) install.packages("seqinr")

if (!require("ape")) install.packages("ape")

Let’s go ahead and run these lines.  The RStudio IDE makes it easy to run code line by line; simply put your cursor at the top of the code and hit the RUN button that’s towards the middle right of the script pane:

Your code is executed line by line if you keep pressing the RUN button, and any results or information are printed out in the CONSOLE pane at the bottom left:

Bioconductor is a little different – you have to install the Bioconductor installer, then install packages using that:

# Biostrings (from Bioconductor) is a bit different to install:

if (!require("BiocManager", quietly = TRUE))

  install.packages("BiocManager")

BiocManager::install(version = "3.16")

BiocManager::install("msa")

Running that will give you a lot of information – and if you have already installed Bioconductor previously you may have to answer a few questions regarding updating existing packages in the console pane:

I just entered “a” for all (don’t use the quotes!) and let it update.

Now that the packages are installed, let’s put some code in the script to load them into memory for the current script:

# Load all required libraries

library(sequinr)

library(ape)

library(Biostrings) # from Bioconductor!

library(msa)

Let’s execute that using the RUN button.  In fact, let’s highlight all of the lines we want to run using our mouse (click and drag) and run them all at once:

Executing this code gives us an ERROR message in the console!

It’s always a good idea to check the console output when you are executing code for the first time – and this is exactly why.  Reading the error message tells us that there is an error in library sequinr.  Why would that be – didn’t we install seqinr?  If you look carefully at the code you’ll see that we did install seqinr but we tried to load sequinr – our first bug, squashed already!  Let’s change that line to say:

library(seqinr)

And if we run that block now, it executes perfectly!

So rule #1 for troubleshooting:  execute your code line-by-line in the IDE and check for error messages.  When you see one, read it carefully and see if you can figure out what went wrong.

RStudio has a nice feature (that most other IDEs have as well) – if you highlight a term (maybe a variable name or a library), it will show you wherever else you have used that term.  That’s a great way to make sure your variable names are correct – or your libraries.  Double-click on the (now fixed) line for loading the seqinr library and you’ll see that the term seqinr is boxed in the lines up above where you told RStudio to load it:

Now we can see that the library we load is indeed the library we installed!

Let’s add a few more lines of code – these will use the ape library to download GenBank sequences as storm them as binary variables.  Go ahead and run these lines after you add them:

# Let's load 3 sequences by accession number using ape - this creates binary

# sequence representations

SAG6220_bin <- read.GenBank("U22939.1", as.character = TRUE)

ATCC30963_bin <- read.GenBank("U22938.1", as.character = TRUE)

Cdysosmos_bin <- read.GenBank("U13985.1", as.character = TRUE)

In addition to looking at the CONSOLE to see if everything ran fine, you should also be able to look at the ENVIRONMENT tab (top right) and see that three variables have been defined, one for each sequence:

We need to convert these binary sequences to a more accessible form, so we’ll take the first part of the binary (the sequence part) and put that in a new variable by adding and running these lines:

# Convert to simple sequences for easier processing

SAG6220_seq <- SAG6220_bin[[1]]

ATCC30963_seq <- ATCC30963_bin[[1]]

Cdysosmos_seq <- Cdysosmos_bin[[1]]

You should now see 3 new variables in the VALUES section of the ENVIRONMENT tab:

Now let’s do some basic sequence analytics.  We could do this in the script, but we’re just getting some descriptive information, so let’s use the console instead.  You can type commands directly in the console – try typing each of the non-commented lines into the console and running it by simply pressing RETURN.  There are errors in each line of code below, see if you can figure them out:

# sequence length

SAG6220_length <- lenth(SAG6220_seq)

# sequence GC content

SAG6220_GC <- GC(SAG6290_seq)

# kmer frequencies for k=3

SAG6220_kmers <- count(SAG6220_seq, "A")

# sequence composition

SAG6220_table <- tabble(SAG6220_seq)

Scroll down to the next page to see what was wrong!

The first line should have produced this output in the console:

> SAG6220_length <- lenth(SAG6220_seq)

Error in lenth(SAG6220_seq) : could not find function "lenth"

Whenever you see an error like “could not find function”, that means that you asked your IDE to load a library/function that it isn’t able to find.  Yes, that’s common sense – but you have to ask yourself WHY it couldn’t find that function.  In this case, it’s pretty obvious that the function is misspelled (it should be length instead of lenth).  But in other cases, you’ll find yourself trying to use a perfectly acceptable function and it won’t run.  There are two reasons for this:

The library containing that function isn’t available (e.g. maybe it’s not installed, or you didn’t invoke it using the library command).

Something about the way the function was run is incorrect, e.g., you didn’t pass the correct parameters, or the variables are cast wrong.

The second line should have produced this output:

> SAG6220_GC <- GC(SAG6290_seq)

Error in GC(SAG6290_seq) : object 'SAG6290_seq' not found

This type of error tells you that the sequencing object you tried to process couldn’t be found.  If you look at your ENVIRONMENT panel, you’ll see three objects with the “_seq” name – and none of the is SAG6290_seq.  So this is just another typo!

The third line should have produced this output:

> SAG6220_kmers <- count(SAG6220_seq, "A")

Error in length - 1 : non-numeric argument to binary operator

This one is a bit harder to figure out.  Whenever you see the term “argument”, you should think of it in terms of a command argument – an “argument” in that context is a parameter you pass to a program.  In this case, we tried to run the COUNT command with parameters SAG6220_seq and “A”.  It sure looks like there is a SAG6220_seq, but there’s no “A” variable.  Let’s dig a bit deeper, using the built in command help function in RStudio (there will be a similar function in any good IDE).

Start by entering the command “count” (no quotes!) into the console, leaving your cursor at the end of the word (no space).  You should see some new popups appear – this is part of RStudio’s autocompletion function:

The function we want is the first one, the plain “count”.  Scroll up to that one and leave it selected:

That’s a bit more useful – it shows us exactly what kind of arguments can be passed to the function!  Notice that you can also press F1 to gain access to the full description of the function in the HELP panel at the bottom right.

Looking at the possible parameters, you can see that we aren’t specifying everything we could – Rstudio (and most other programming languages) have predefined defaults that they will use for functions, so you only have to pass the parameters that you want that are different from the defaults.  In this case, we are trying to count k-mers, so we pass the sequence and the wordsize.  As it turns out, “A” is not a valid wordsize – trying running the command like this instead:

SAG6220_kmers <- count(SAG6220_seq, 3)

See if you can figure out the problem with the 4th line on your own – it’s similar to one of the cases we’ve already examined.

There are many, many other ways that code can go wrong, but hopefully you’ve gained some insight – and some tools to use – to try and figure more complex issues out on your own.  One last thing I want to point out about RStudio (and a few other IDEs):  The interface itself will often highlight regions where it thinks there may be problems.  For example, RStudio will put a red “X” at the left of lines of code that it thinks are problematic, e.g., if it can tell there is a syntax error:

Troubleshooting Data

Sometimes it’s not your code that’s the problem – sometimes the data comes with complications that you have to correct for.  For example, does the CSV you are importing have headers?  If so you’ll want to read it in so that the header labels don’t become part of the “data”.  Does your data have missing data?  You may have to read it in differently or fill in gaps.  Does your data have inconsistencies in spelling (e.g., a data column may only have two states, yes or no, but what if they are encoded using Y, y, N, and n, which in some programming languages equals four states instead of two?  Even more confusing – is the data being read in as the correct type of variable (e.g., numerical instead of character or text)?  And lastly – sometimes you’ll get data where it isn’t exactly clear what each column (or row) of data actually is – so you’ll have to do some sleuthing to figure it all out.

You will eventually run into each and every one of these problems, and the first symptom will simply be “it doesn’t work”.  Let’s look at some ways that you can suss out the problem without losing your sanity.

Let’s start by reading in a “simple” csv file.  In the CLASS RESOURCES you should find a file called week6.csv, download that and save it in the same location as the R script you are currently working on.  Now let’s try to read it in.  Add the following line to your script and run it:

newData <- read.csv(file='week6.csv, header = FALSE)

Did you notice the red “x” RStudio put at the line?  Hopefully you figured out that the closing single quote was missing and that the file wouldn’t load – try again with this version:

newData <- read.csv(file='week6.csv', header = FALSE)

After running this line you’ll see a new entry in the DATA section of the ENVIRONMENT panel at the top right.  Go ahead and click on the line for newData in the ENVIRONMENT tab to view the data:

We aren’t showing the entire table here, just the top 7 lines.  You can see that our table has 4 columns, clearly labeled  - but the first row contains text representing the actual headers from the CSV.  Let’s see what kind of trouble that actually causes.  In the ENVIRONMENT tab, click on the blue arrow to the left of newData to show more information about the data:

As you can see, all of the data imported as character data (shown by the “chr” in each line).  That means that you won’t be able to do any functions on any of the numerical data like AGE.  Also, the column headers were arbitrarily labeled as V1 through V4 – not exactly what we wanted.  So you can imagine that running any downstream analysis on this data would fail if you were expecting anything other than character data!

This is actually an important trick – looking at your data in the data browser as we did above will show you the contents of the data, but it won’t show you anything about the way the data was cast – meaning what type of variable it is (numerical, character, etc.).  But we can get that information from the ENVIRONMENT tab!  You’ll need to do know how to do this, since even importing a table the correct way may lead to an improper data cast – and you’ll also have to know how to fix it.

Let’s reimport the data with a small difference – we’ll tell the read.csv function that the data does have a header:

newData <- read.csv(file='week6.csv', header = TRUE)

Now if we look at the ENVIRONMENT tab we can see that numerical values have been cast as integers, and character values have been cast as characters:

What if the file format doesn’t match?  Most programs are very accommodating, to the point where they trust you to make the correct decision.  For example – what happens if you try to read that csv file as a table?  Go ahead and try it:

newData2 <- read.table(file='week6.csv', header = TRUE)

Viewing the ENVIRONMENT tab for this we see:

Even worse, if we click on the name to view the data:

There is only one column – all of the data from the four columns in the csv were read into a single column, since we didn’t let R know that this was a column separated value (CSV) file!  Clearly this won’t work – good thing we checked!

Checking the format of your data – both before (to see that it is or isn’t what you think it is, e.g., a table vs a csv file) and after importing (to see if you have the correct number and type of variables) is an easy way to double-check your work.

Even when everything seems to go correctly, you will eventually find yourself having to re-cast data as a different type of variable.  Luckily R has a variety of functions to help you do that!  Before we learn how to recast data, let’s review the common types of variables in R:

The first two of these are pretty self-explanatory – numeric is any number, integer is only integers (no complex numbers).  Character is also pretty straight forward – it can be any character you can type (numbers, letters, and symbols), but whatever is classified as Character can ONLY be manipulated as a character – in other words, you can put numbers into a variable that is classified as Character, but you won’t be able to manipulate them as numbers unless you recast the variable (or values).  More on that later.

The remaining two are a bit more fiddly to explain:

Factor is defined as “a categorical text value”, which basically means that it’s a text value but with a limited number values – e.g., “black”, “white”, “red”, “green”, “blue”.  A factor can be used for labels, conditions, classes, etc., but not for general text (which is what Character is for).

Logical variables can have only two values – true or false.  You can use them to classify numerical values based on a threshold, e.g., if you are looking at blood pressure and want to quickly classify people as hypertensive or normal you could program a line so that any reading above or below a certain value gets a Y (for yes) or N (for no) added to a separate column.  This makes it easier to re-classify variables later (if, for example, you change the threshold values).

Let’s look at a few examples of recasting variables:

Character to factor:

In the newData variable you created, there is a variable called CLASS that stores Y or N (or sometimes y or n).  Since this is a variable where we are simply storing Y (for yes) or N (for no), we could recast this is either a character or a logical variable to make it easier to work with later.  First, let’s look at the contents of this column again – type the following line into the console and press RETURN:

newData$CLASS

This command lists the CLASS column from the newData variable.  As you can see, it contains only Y, N, and a single lower-case y (which is a typo and will cause trouble later – don’t worry, I’ll show you how to fix that as well).  For now, let’s use the following command to display this data as a factor:

as.factor(newData$CLASS)

You may have noticed that after you typed the “$”, the console gave you a popup – this contains all of the columns in the newData dataset that you could choose – another way IDEs are helpful!

Once you have the line entered, press RETURN.  You will immediately see that you have FACTOR output, with three levels – thanks to that pesky lower-case Y.  So before we make this change permanent let’s fix that problem using this command:

toupper(newData$CLASS)

This prints out the contents of newData$CLASS as all upper case!  We now have all of the tools we need to change the variable and fix the case problems, so let’s tie it all together into one command:

	as.factor(toupper(newData$CLASS))

As you can see, we are first converting all of the contents of the CLASS column to upper case, then converting them to factor – using parentheses to separate the actions.  MAKE SURE YOU UNDERSTAND HOW THIS WORKS before moving on!

So far all we’ve done is to see that we can change the variable type, and display the output in the console.  Now let’s save those changes into the existing data frame (which is what we call the entire collection of data in “newData”).  Before we make the changes, let’s first expand the contents of newData in the ENVIRONMENT tab by making sure that the blue circle/white arrow is pointing down, as seen in the image to the right:

Once you can see the variables in the ENVIRONMENT panel, go ahead and add this line to your code and run it, paying attention to the CLASS portion of newData in the ENVIRONMENT panel:

newData$CLASS <- as.factor(toupper(newData$CLASS))

The CLASS column in newData has now been changed from character (chr) to Factor – with 2 levels (Y and N) and not 3 (Y, y, and N).  Easy, right?  

It’s important to point out that this was easy ONLY because we knew exactly what the problem was – in this case, our variable was the wrong type.  If you pay attention to the error codes you receive when running your code line by line, you should be able to pick up on the things like variables being cast as the wrong type, and using the ENVIRONMENT panel to check out the variable types you have you will be able to quickly figure out where things went wrong.

As you may be able to guess, there are commands to change a variable to any other type:

as.integer

as.numeric

as.logical

as.character

as.factor

Your mission will be trying to figure out when to use one of them, and which one to use.  Most of the variable casting errors you will run into when importing data will inevitably result in a variable miscast as character – so pay attention to that ENVIRONMENT panel!

Even when your data imports correctly you may run into issues using it.  Keep in mind that bioinformatics is a very academic field, and many of the best/most popular software packages and data sets you can get were created and maintained under academic circumstances.  This means that your mileage may vary in terms of the quality of both – there may be bugs, or issues installing or running software packages, and data sets may be messy in a variety of ways (e.g., missing data, poorly labelled columns/row, etc).  You may have to stare at your data for a long time to figure out what kind of variable it should be cast as, or what the data even means.  Let’s take a look at an example from the real world using two different diabetes datasets.  The first one we will look at was downloaded from Kaggle and is called kaggle_diabetes.csv (you will find it in the COURSE RESOURCES).  If we use our standard read.csv command:

kaggle_diabetes_data <- read.csv(file='kaggle_diabetes.csv')

The data is loaded in with no errors – let’s click on the name diabetes_data in the ENVIRONMENT panel to see what it looks like:

I’m only showing you the first 10 rows of data, but it looks nice – all the columns are clearly labeled, but Kaggle itself does not include any additional information about the data, so without emailing the person who provided it (something you’re likely to have to do at least once in your life if you use an online database) we can’t figure out more about it.  The biggest question you may have is: what do the values (0 and 1) in OUTCOME mean?  Chances are that OUTCOME indicates whether or not the individual represented in that row actually had diabetes, but again, without additional information, we won’t know.

As an aside – this is a pretty good dataset; if you look at the ENVIRONMENT panel and expand diabetes_data you’ll see that all of the variables seem to have been cast correctly.  The only one I question is OUTCOME, which could have been read in as a factor, but integer is okay.

Let’s take a look at another diabetes dataset – this one from Mendeley Data.  It’s called mendeley_diabetes.csv and is also in the COURSE RESOURCES.  Let’s load that with:

mendeley_diabetes_data <- read.csv(file='mendeley_diabetes.csv')

Again this data loads readily, but if we look at the way the various columns were cast we see some issues:

We have a GENDER field that has been cast as character, and a CLASS field that has also been cast as character.  Reading through the data, you can see that these could be factors, but there are some capitalization errors that would have to be fixed first.

More distressingly – what exactly does the CLASS variable contain?  There are three values (if you ignore capitalization):  Y, N, and P.  If we go back to the source at Mendeley, they tell us that Y = diabetic, N = not diabetic, and P = predicted to be diabetic.  But without paying close attention to the text, you would never know!

The last thing we will cover is one of the more annoying things about R and RStudio for beginners – the Working Directory.  The Working Directory (WD) is defined as the default location where R will look for files to load, as well as where it will save any files you create.  It is typically set as your “home” directory – unless you load an R script by double-clicking on it, in which case the WD is set as the location of that R script.  This is useful in that you can save an R script to a folder or other location that contains your data files and then load the files with a relative path instead of an absolute path – for example:

Absolute Path:  file=’C:\Users\wolfgang\Desktop\RStuff\data.csv’

Relative Path:  file=’data.csv’

You can see why the relative path is easier – to remember as well as to use.

You can also use the FILES tab in the bottom right of RStudio to manually set the WD.  Looking at the files tab you will see a hierarchical directory listing of where the FILES tab is currently pointing.  You can click anywhere on that line (e.g. on the IN PROGRESS on the left) to jump to that directory.  Alternately, at the far right of that line you’ll see an ellipses (three dots in a row:  …).  Click on that and your system will pop up a directory navigator:

Use the directory navigator to go to any directory you would like to use as your WD.  Then, all you have to do to set it for the current session is to use the gear icon above the directory line:

To set the directory that is currently displayed in the FILES tab as the WD, just select “Set as Working Directory” here.  Easy, right?

You can also use commands in R to set and load the WD.  You can explore those on your own, but I’ll give you a start with this command:

getwd()

The getwd() command simply returns whatever the current WD is.  You can display this in the console, assign it to a variable, anything you like.  Most importantly you can use it to verify that the WD is what you think it should be!

That’s it for this week.  Make sure to check out the OERs as well – see you in class!



--- SOURCE: Week 7 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week  7 Lecture – Introduction to Linux & The Command Line

By now you’ve seen that many of the algorithms we use in bioinformatics are not your “standard” sort of application with a graphical user interface – in fact, most hardcore bioinformatics gets done on the command line – and that typically means a linux (or MacOS, which is linux under the hood) operating system.  This week you will become familiar with linux and some of the basics of working via the command line.

To begin with you will need access to a computer running linux.  There are many flavors of linux, and while they have differences in the way that they are maintained and updated, the basics of the command line are common to all of them.  For purposes of this class we will be using Ubuntu, which is a “Debian” based linux that is one of the easiest to install and use.  

Your Personal Linux Machine

Don’t panic – you do not need a new computer, or even to replace the operating system on your computer, to install Ubuntu.  You can, if you wish, install Ubuntu “next to” your Windows operating system (creating a dual-boot system), but that is completely optional and in fact I would recommend against it unless you are very comfortable tweaking your computer.  Instead, we will use a hosted linux solution provided by UMGC through our Lab Broker.  

To access your personal hosted Linux machine, you will need to go to the COURSE RESOURCES section of the classroom and look for "Lab Broker":

When you click on the Lab Broker section in Course Resources, you'll see this on your screen:

You can click on the "Click Here to Access LabBroker" link, although you might want to first click on the "How to Access your Hosted Linux System" link and watch the video first if you have never used Lab Broker before.  A login and password combination will be provided to you separately.

NOTE:  Linux does not show any activity when entering a password (e.g., it won’t show you an asterisk for every character you enter).  So if you start typing your password and don’t see anything, nothing is broken – just continue and hit RETURN!

FIRST STEPS:  UPDATING LINUX

Once you have access to your Hosted Linux, you'll probably want to make sure that all of the software on it is up to date.  Ubuntu has a nice built-in update system called “apt”.  It’s good to make sure that you have the latest versions of all of the components of linux, so the first thing we are going to do is learn how to use apt a bit.  This is also going to be useful in the next step, where you will learn how to install some software (which is referred to as installing packages).

Log in using the supplied.  You will be taken to the Ubuntu desktop (note that this may be different if you are using the UMGC hosted Linux – you will not have any icons or a visible dock):

Figure 1:  Ubuntu 18 Desktop

Figure 2:  Ubuntu 20 Desktop

If you are using the UMGC Hosted Linux you will see this:

Ubuntu gives you a nice Desktop, just like Windows or Mac.  But we want to work on the command line!  Notice that there is a dock at the bottom – the terminal application is the second from the left.  Go ahead and click that.  

NOTE:  if you go “full screen” on the desktop and lose the dock you can get it back by clicking the ACTIVITIES button in the top left).

Notice that not only did a terminal window open, but there is an icon for the terminal in the dock on the left.  You can right-click on that icon to make it stay in the dock permanently so that you can access the terminal without the right-click next time.

Now (left)click inside the terminal to make it the active window and type:

sudo apt update

and press return.  The sudo command tells Ubuntu that you want to run the apt update command with administrative privileges; in most systems you’d have to enter your password after that, but in this particular install you are already the admin.  Ubuntu looks for updates and if there are any it tells you.  You can install the updates by typing:

sudo apt upgrade

and pressing return.  Ubuntu will list the updates and then ask you if you want to proceed; enter ‘y’ to continue.

NOTE:  linux lets you string commands together commands using “&&”; for example, you could have simply typed:

sudo apt update && sudo apt upgrade -y

This will execute first the apt update, then the apt upgrade with a default answer of “y”.  

The update process may take some time – go grab some coffee, or if you’re really gung-ho, go to Amazon or your library (or a used bookstore) and get a copy of “Linux in a nutshell” – it’s a great command reference for linux and you can usually get it used for about $5.  You should also be aware that the screen saver may kick in and blank the Ubuntu screen – don’t worry, just click in the window to continue.

Now you’ve updated linux – let’s do some more interesting things.

INSTALLING PACKAGES

Earlier in this course we learned about some basic bioinformatics packages like samtools and minimap2.  While you won’t be able to use minimap2 to map sequences against the human genome with this particular system (you would need a linux machine with at least 32Gb of RAM to do that), you can map smaller datasets.  Since you’ll need that for this week’s homework, let’s go ahead and install samtools and minimap2.  In your terminal window type:

sudo apt install samtools

and press return.  Enter “y” when asked, and Ubuntu will go ahead and install samtools.  After the install, go ahead and type:

samtools

into the window and press return – notice that if you execute a command without any of the additional information (e.g. the flags, file inputs, etc), linux will usually just give you a quick “help” file output.

Go ahead and install minimap2 the same way.  Depending on the version of linux you are using, you may have to install minimap2 in a different way:

curl -L https://github.com/lh3/minimap2/releases/download/v2.20/minimap2-2.20_x64-linux.tar.bz2 | tar jxf -

sudo cp minimap2-2.20_x64-linux/{minimap2,k8,paftools.js} /usr/bin

You can (and should) read more about minimap2 on it's github page at https://github.com/lh3/minimap2.  

NOTE:  while it seems easy and intuitive to install and run applications in linux, it isn’t always – you have to know the exact name that it is expecting.  For example, if you wanted to install Mr. Bayes, the Bayesian phylogenetic reconstruction software, you would type sudo apt install mrbayes to install it, but at the command line you would simply type mb and return to launch it.

BASIC LINUX NAVIGATION:  pwd, cd, ls, mkdir, mv, rm, which, df, ln -s, and alias

Navigating directories and files is a little different from the command line.  Let’s start by seeing where exactly we are in the terminal by typing the following command and pressing return:

pwd

While you might guess that this is a password command, pwd actually stands for print working directory.  This will output the directory that you are currently in, which is /home/StudentFirst (your home directory).

The command for changing directories is cd, but there are many ways to use cd.  For example, if you type:

cd ..

followed by the pwd command, you will see that you are now in the /home directory.  So using cd to change directories followed by “..” moves you up one level in the hierarchical directory structure.  You can’t do any work here since you don’t have permissions, so go ahead and type cd StudentFirst and press return to go back home.  You could also just type this (from any directory) to jump immediately home:

cd ~

Now that you are in your home directory (recall that you logged in as user ubuntu), let’s see what files you have in there.  Use this command:

ls

As you can see, ls is the list command.  Again there are variations in what you can do with this command.  For example, try ls -al instead and you will list everything in the directory, including invisible files and directories (they start with a “.”), as well as their permissions, ownership, creation date, etc.

Now let’s create a new directory that you can work in.  The mkdir command will create a directory:

mkdir homework4

Notice how I combined homework and 4 – that’s because linux does not like spaces in words – in fact, it would interpret mkdir homework 4  as a request to create two separate directories, one called “homework” and another called “4”.  To make things neater we could have called the directory “homework_4” – let’s use another command, mv, to do that.

mv homework4 homework_4

The mv command is the move command.  It is really a renaming command, but move makes sense too.  Notice that we passed the mv command two inputs – one is the existing named item, and the second is the name we are moving it too.  Many linux commands work this way – and additionally you can add flags and other controls on the command line….more about that later.

Now let’s remove our new folder – essentially deleting it.  In linux the rm command removes a file, whereas rmdir removes a directory.  Since homework_4 is a directory let’s do this:

rmdir homework_4

Alternately, you could use a flag and the rm command:

rm -r homework_4

The -r flag tells rm to act recursively, removing the item and anything contained inside of it.  Most linux commands support the use of flags and other ways of controlling their function.

There’s another trick that you should be ready for now.  While your cursor is in the terminal, press the “up” arrow on your keyboard.  You’ll see that the last command you entered is displayed, ready to be run again.  You can edit this command, or you can keep pressing the “up” arrow and go back through the entire history of the commands you’ve entered this session.  This is a nice way to repeat commands with minor changes, or to see what you’ve been doing.

Now let’s imagine we are about to start writing a large python program.  First let’s make sure we have python3 installed.  We could just try executing python, but instead let’s use the which command:

which python3

After typing the above (you should know by now to press return at the end of commands) you should see this line:

/usr/bin/python3

The which command asks linux to tell us which program we will run if we give it a program name.  You may ask why this is useful, and it’s a fair question.  The answer is that linux (like most other operating systems) can end up having lots of different applications installed, and not always where you expect them to be – so the command which python3 lets us know that (a) python3 is in fact installed, and (b) we will be using the python3 that has it’s executable stored in /usr/bin/.

Now let’s make sure we have enough free disk space to write our program.  We can do that using the df command:

df -h

Notice that we used another flag – in this case, the -h flag, which tells our application df (or disk free space) to report the results in human-readable format.  There’s a lot of information here, and we really only care about the entry at /dev/mapper/vgubuntu-root, which is the “hard drive” that we are using for ubuntu.  If you look at the column marked “Avail” you will see that in my system I have 86Gb free out of 97Gb:

Let’s pull a few more tricks out to make this easier to read – try entering this command:

df -h | grep “/dev/mapper/”

This gives us just the single line from the above:

How did this work?  We took our same command, df -h, and used the “|” character (the straight up-and-down line, which on my keyboard that is the shift version of “\”) to “pipe” the output of df to grep.  Grep is a search function that we’ll go into later, but essentially we searched the output of df -h looking specifically for a line that included “/dev/mapper/” – and grep returned the single line for us.

You can use “|” to pipe the output of any command to the input of any other command.  Very useful!

If we wanted to now start using python to write our program, we would type the following command:

python3

This would bring up the python command line interface:

While this is fine, I know that I don’t always remember to put the “3” at the end of “python” – I’d rather just be able to run python by typing “python”.  Luckily we can customize linux to enable us to do that – and more.

To quit python, enter the following command:

exit()

Now check your current directory using the pwd command.  Notice that you are still in your “home” directory.  Just in case you have explored the filesystem a bit, here’s a command that will instantly return you to your home directory, regardless of where you are:

cd ~

We used the change directory, or cd command, just like before – but this time we told it we wanted to go to the “~” location.  That tilde tells cd to take you home!  You could just as easily have typed the whole path, like this:

cd /home/StudentFirst

Recall that “StudentFirst” is the name of the user we logged in as.  On my personal linux machine, for example, my home directory is actually “/home/wolfgang/”.

Now that we know we are in our home directory, let’s take a look at all of the contents again using this command:

ls -al

Notice that there is a hidden file called “.bashrc” – that’s where linux stores configuration data for the bash terminal.  Let’s append a line to the bottom of this file – without using an editor (we’ll learn how to use the terminal to edit files later):

echo alias python=’python3’ >> .bashrc

There’s a lot going on – let’s break this down.

echo

This command repeats back whatever you type after it, so echo alias python=’python3’ simply makes the terminal print out “alias python=’python3’.  You may have noticed that nothing was printed out, though – that’s because of these characters:

>>

This pattern of characters is known as the “append” command.  It basically redirects the terminal output and appends it to whatever file you list after it.  So that’s why the terminal didn’t print anything from the echo command; it was redirected and appended to the file .bashrc

So what does this do?  It adds a command at the bottom of your bash configuration file that tells it that whenever you type the word python you really mean python3.  So now, instead of having to type python3, you can simply type python to launch the python3 application.

Let’s see if it worked.  Exit the terminal by clicking on the  button at the top right of the terminal window.  Now relaunch it (with a right-click on the desktop or using the toolbar, if you added it) – this reloads the .bashrc file, so your alias command should now be active.  Enter the command:

python

And if you see python3 launch, it worked!  

There’s one last basic linux command we’re going to learn this week, and that’s the autocomplete function.  Make sure you are in your home directory, then type the following (but do NOT press RETURN):

cd De

Now press the TAB button.  Linux will autocomplete to the only directory you have that starts with “De”, which is “Desktop”.  But before you press RETURN, let’s change the command.  Delete the last “e” so that your command looks like this (again do not press RETURN):

cd D

If you press TAB once, nothing happens – that’s because you have three different directories that start with “D”:  Desktop, Documents, and Downloads.  Press TAB twice, though, and you’ll see a list of exactly those three folders.

Using TAB as an autocomplete works inside of bash (the terminal), and you can use it for filenames, folder names, or for commands.  For example if you type py and hit tab twice, you’ll see all of the commands that being with py.

Go ahead and exit your hosted Linux machine (unless you want to play around some more on your own).  Next week we will learn how to do a variety of file viewing and manipulation in linux – essential stuff for a bioinformaticist.



--- SOURCE: Week 8 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week  8 Lecture – Introduction to linux, Part II:  Bioinformatics in Bash

Last week you learned your way around the basics of Bash, the linux terminal.  This week we’ll build on those skills and start working on bioinformatics data files using linux.

As we have discussed previously, many of the bioinformatics data files you will encounter are simply flat text files – meaning they have no complex structure (like a Word file).  This means they are very easy to view and manipulate, and linux has some of the best tools for viewing and manipulating text files.  Let’s start with some simple examples.

First, you'll want to download the sample file "sample.vcf" from the COURSE RESOURCES section of the classroom, then upload the sample file to your hosted linux.  You can do this using the button at the top of the screen:

Pressing this will give you a series of buttons – one of which allows you to upload:

Place the sample.vcf file on your Desktop, then using the terminal navigate to the desktop:

cd Desktop

VIEWING FILES IN LINUX

Let’s take a look at what’s inside our file.  Enter this command into the terminal:

head sample.vcf

The head command displays the first 10 lines of the file you list after the command.  Note that the lines will wrap, so it might be a little hard to read.  You can resize your terminal window to try to display the lines more clearly:

The head command isn’t limited to displaying the first 10 lines – for example, if we use the -n flag, we can tell it to display as many – or as few – lines as you like:

head -n20 sample.vcf

This tells head to display the first 20 lines of sample.vcf.  When you do this you’ll see that you are now viewing more than just the headers (the lines starting with “##”) – and the data is very “messy” to read in the terminal because of the large number of columns.  Still, the head command is useful to peek into a file and see the contents.

Equally useful is the tail command.  As you may have guessed, the tail command displays the last 10 lines of the file, by default – and you can use the same flag (-n) to change this:

tail -n15 sample.vcf

My favorite use of the tail command is actually to watch a log file, or a data file, as it gets written to – you can monitor the new lines being added in real time.  This isn’t something we’re going to do today, but if you’re curious, you can try added the -f (for follow) command to tail when you’re viewing a file that’s currently being written to.

So head and tail let us see the beginning and end of a file.  We can also view the entire file all at once by using the cat command:

cat sample.vcf

When you enter the above command, the entire contents of the sample.vcf file go whizzing by in the terminal.  It’s not the best way to read a large file, but it is a great way to send a file somewhere else – for example, if you want to merge two files, you would use cat to concatenate them into a new file:

cat file1 file2 > merged_file.txt

You can also use cat to send a file to another application for processing using the pipe symbol we learned earlier (“|”).  But we’re getting ahead of ourselves – how can we more easily read a large file like sample.vcf?  Let’s use the more command:

more sample.vcf

The more command displays the file one “page” at a time.  This isn’t a standard page, it’s just sized to fill however large your terminal window is.  To see the next page, you can press the SPACE bar, or to just display the next line, press RETURN.  To exit, so that you don’t have to scroll all the way to the bottom of the document, press the “Q” button.

The more command is nicer than cat, but there’s another viewing command you should know, and that is less.  Enter the following command:

less sample.vcf

Similar to more, less will display a the file one “page” at a time.  What makes less nicer, however, is that in addition to allowing you to move to the next line with RETURN, or the next page with SPACE, you can use the up and down arrow keys to smoothly scroll up or down through the document.  The “Q” button again exits the program.

EDITING FILES IN LINUX

There are as many ways to edit a file in linux as there are ways to view one.  You can certainly use a graphical text editor like gedit or text editor, but in bioinformatics the majority of the time you are working you will be in the command line – for example, when you are connected to a remote server, like a supercomputer.  So let’s use a command line text editor – my favorite is nano.

To begin, enter the following command:

nano sample.vcf

The sample.vcf files opens in the nano screen editor:

The nano editor is fairly easy to use, but it’s not quite like a word processor.  To move the cursor to a different line – say where you want to make a change – you can use the arrow keys (all four – up, down, left, and right).  Once the cursor is where you want it, simply start typing (or deleting, etc.).  To exit out of the editor, you can use the CONTROL+X buttons (if you look at the bottom of the UI you’ll see that there are a series of commands that you can access using CONTROL plus some other key).  The nano editor is powerful, but since all of the features require that you know the key combinations, you’ll just have to practice using it to get comfortable with all of them.  Luckily you can “cheat” and use the manual – more on that later when we talk about the man command.  For now, exit out of Nano and, if you made changes, do not save them.

PARSING FILES IN LINUX

The ability to write custom filter commands is what makes linux incredibly useful to bioinformatics.  You’ve taken several looks at the sample.vcf file by now, so you’ve seen how complex it is – not just in terms of size, but the number of columns and their contents.  There are three commands that we will use to make this data more manageable, and to pull out the interesting elements:  grep, awk, and sed.

The grep command is used to search for global regular expressions – at it’s simplest you can use grep to just look for words and phrases, although it’s far more powerful than just a search.  

Let’s do a simple example – let’s say you are looking for any mentions of a particular SNP (single nucleotide polymorphism) in your variant caller file – let’s say the SNP is rs56190854.  You can use grep to search for it by name by typing this command:

grep rs56190854 sample.vcf

This command reveals that there is one line that contains that SNP ID in the entire file.  It doesn’t look like one line because it wraps, but we can prove it by using another command that counts the number of lines in our output.  First let’s clear the screen by using the clear command:

clear

Now let’s pass the results of the above grep command to wc -l, which is the counter:

grep rs56190854 sample.vcf | wc -l

Did you remember that you can use the up arrow to return to a previous command?  If you use the up arrow to return to the original grep command, you can just add the additional terms and press return.  This returns the value of 1, so there is just one instance of rs56190854!  The wc -l command is actually the word count command using the “-l” flag to count lines instead of words.  Very handy, isn’t it?

The grep command is useful for finding specific references to things.  We can also use it to get the context of that item by telling it to return additional lines before and/or after the line containing our search term.  If we enter this command:

grep -A4 rs56190854 sample.vcf

The “-A4” flag tells grep to include the 4 lines following the result (or results, if there are more than one).  In a similar fashion you would use the “-B” flag to show additional lines prior to the match.  And you can always do both:

grep -A1 -B1 rs56190854 sample.vcf

In the above examples we were looking for a specific item, and grep returned only that one item.  However, if there had been a SNP ID that was identical to our query but longer, grep would have found both of them – e.g. rs561908547 would also have shown up (notice that it is the same but with one additional character, a 7, at the end).  That’s because grep looks for the pattern, not the exact match!  

Let’s test this by searching for something we know is represented multiple times:

﻿grep "INFO=<ID" sample.vcf

The “INFO=<ID” string is found multiple times.  Note that we had to include the search term inside non-curly parentheses so that grep would not use the search characters as command terms!

Let’s do another example:

﻿grep "INFO=<ID=N" sample.vcf

This returns even fewer results, because we are narrowing our search down!

You can also use grep to return the inverse of your search – in other word, find every line that does not contain your result using the “-v” flag:

Lastly, we can use grep to search through not just a single file, but an entire directory of files:

﻿grep -v PASS sample.vcf

In this example we searched for lines that didn’t include “PASS”.  What we saw returned were only the headers!  The software that generated the .vcf file only included variations that passed their filter, so that is what we would expect.

NOTE:  you have probably noticed by now that linux is case sensitive – for example, using a “-v” for a flag is very different from using “-V”, and if you had searched for “pass” instead of “PASS” in the above example you would have seen the entire file zip by – since “pass” isn’t present anywhere, only “PASS”.

Let’s move on to our next command, awk.  The awk command is a powerful text processor that works really well with data tables, whether they are comma or tab-delimited.  As you recall, many bioinformatics data formats are of this type, so you’ll be spending a lot of your time in awk.  Unlike most linux commands, awk isn’t named for its function; instead the letters “a”, “w”, and “k” refer to the last names of the developers who wrote awk:  Aho, Weinberger, and Kernighan.

Our sample.vcf file is a column-based file that is tab-delimited, which makes it a perfect data file to use awk on.  If you recall, one of the header lines in sample.vcf told us what the column headers are.  For our purposes today we’re only going to use the first 7, which are:

CHROM	POS	ID	REF	ALT	QUAL	FILTER

Let’s start by walking through what these columns mean.  From the samtools VCF specifications (located at https://samtools.github.io/hts-specs/VCFv4.2.pdf) we have the following:

CHROM - chromosome: An identifier from the reference genome or an angle-bracketed ID String (“<ID>”) pointing to a contig in the assembly file (cf. the ##assembly line in the header). All entries for a specific CHROM should form a contiguous block within the VCF file. (String, no white-space permitted, Required).

POS - position: The reference position, with the 1st base having position 1. Positions are sorted numerically, in increasing order, within each reference sequence CHROM. It is permitted to have multiple records with the same POS. Telomeres are indicated by using positions 0 or N+1, where N is the length of the corresponding chromosome or contig. (Integer, Required)

ID - identifier: Semi-colon separated list of unique identifiers where available. If this is a dbSNP variant it is encouraged to use the rs number(s). No identifier should be present in more than one data record. If there is no identifier available, then the missing value should be used. (String, no white-space or semi-colons permitted)

REF - reference base(s): Each base must be one of A,C,G,T,N (case insensitive). Multiple bases are permitted. The value in the POS field refers to the position of the first base in the String. For simple insertions and deletions in which either the REF or one of the ALT alleles would otherwise be null/empty, the REF and ALT Strings must include the base before the event (which must be reflected in the POS field), unless the event occurs at position 1 on the contig in which case it must include the base after the event; this padding base is not required (although it is permitted) for e.g. complex substitutions or other events where all alleles have at least one base represented in their Strings. If any of the ALT alleles is a symbolic allele (an angle-bracketed ID String “<ID>”) then the padding base is required and POS denotes the coordinate of the base preceding the polymorphism. Tools processing VCF files are not required to preserve case in the allele Strings. (String, Required).

ALT - alternate base(s): Comma separated list of alternate non-reference alleles. These alleles do not have to be called in any of the samples. Options are base Strings made up of the bases A,C,G,T,N,*, (case insensitive) or an angle-bracketed ID String (“<ID>”) or a breakend replacement string as described in the section on breakends. The ‘*’ allele is reserved to indicate that the allele is missing due to a upstream deletion. If there are no alternative alleles, then the missing value should be used. Tools processing VCF files are not required to preserve case in the allele String, except for IDs, which are case sensitive. (String; no whitespace, commas, or angle-brackets are permitted in the ID String itself)

QUAL - quality: Phred-scaled quality score for the assertion made in ALT. i.e. −10log10 prob(call in ALT is wrong). If ALT is ‘.’ (no variant) then this is −10log10 prob(variant), and if ALT is not ‘.’ this is −10log10 prob(no variant). If unknown, the missing value should be specified. (Numeric)

FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. Otherwise, if the site has not passed all filters, a semicolon-separated list of codes for filters that fail. e.g. “q10;s50” might indicate that at this site the quality is below 10 and the number of samples with data is below 50% of the total number of samples. ‘0’ is reserved and should not be used as a filter String. If filters have not been applied, then this field should be set to the missing value. (String, no white-space or semi-colons permitted)

Let’s do some simple examples.  Since awk lets you work on columns, we could, for example, find all variants on Chromosome 1 by taking advantage of the chromosome column:

﻿	awk '$1 == 1 {print $0}' sample.vcf

An awk command always has this basic form:  

awk '<optional qualifiers>{<actions>}' filename.ext

In our example, we told awk that anywhere in the file sample.vcf that the first column ($1) had a value of 1, we should print the entire line ($0).  You’ll notice that there were very many matches for chromosome 1, so let’s narrow our search down a bit by only looking at the half-million bases of chromosome 1:

awk '$1 == 1 && $2 < 500000 {print $0}' sample.vcf

In this example, we added a second qualifier and used the “&&” operator to combine them, meaning that both qualifiers had to be met.  Let’s compare the two statements and see how the numbers changed using a trick we learned above – piping the output to wc -l:

	awk '$1 == 1 {print $0}' sample.vcf | wc -l

awk '$1 == 1 && $2 < 500000 {print $0}' sample.vcf | wc -l

The first statement yields 66 lines; the second only 1.

Let’s do a different sort of example – let’s see how many of the variants that were called in this genomic analysis are the result of a “A” to “AG” mutation.  From the description of the VCF format above, we know that the reference (expected) genomic value is stored in column 4, and the alternate (the variant that was detected) is stored in column 5.  So this awk command should work:

﻿	awk '$4 == "A" && $5 == "AG" {print $0}' sample.vcf

We get several of this type of mutation – 4, to be exact.  Let’s change it up a bit – instead of printing out the entire line, let’s just print out the chromosome number for this type of mutation:

awk '$4 == "A" && $5 == "AG" {print $1}' sample.vcf

The print command inside the curly brackets can do much more than print a complete line (which is what $0 is).  In fact, we could even print out the chromosome and the position:

awk '$4 == "A" && $5 == "AG" {print $1, $2}' sample.vcf

Let’s do two more things – let’s output the above results to a file, and then let’s see if we can determine (without counting manually) how many of these mutations there are on each chromosome.

Creating a file from the terminal output is easy – we just use the “>”, or redirect, command:

awk '$4 == "A" && $5 == "AG" {print $1, $2}' sample.vcf > positions.txt

After running this command you’ll have a new file, positions.txt – use one of the commands you learned earlier to view this file before moving on.

Now let’s figure out how many ATG to A mutations there are in each chromosome.  There are many ways to do this – we could even have added another qualifier to our original statement and used wc -l to find it, but since we created an intermediary file, let’s use that since it’s smaller:

awk '$1==1 {count++} END{print count}' positions.txt

This tells awk that whenever column one ($1) has a value of 1, increment the counter named count.  We have to add an END statement to tell it that we are done, then the print command is used to print the value of the counter.  When you run this command, you’ll see that there are 4 AG to A mutations on chromosome 1.  Changing this to reflect the count on chromosome 2:

awk '$1==2 {count++} END{print count}' positions.txt

And the answer is 0.

We could have written this all in one line, piping the output of the original awk statement that created positions.txt to the awk statement that did the counting, adding separate variables for each chromosome, but I like using intermediate files – it makes checking for bugs and errors much easier.

There’s far much more you can do with awk – it can do mathematical operations within columns, so you could add or average values, but I’ll let you explore that on your own.  Now let’s move on to the last of the parsing tools we’re going to talk about – sed.

The sed command is not so much a parser as it is an editor.  In fact, sed stands for stream editor, because it works on an “input stream”, which is a fancy of way saying it will work on files or the output of other commands.    Sed is useful when you need to search and replace within a document – which can let you reformat a document (e.g. replace tabs with commas or vice versa) or change the text to something more readable (e.g. replace “chr” with “chromosome”).  You can also used sed to edit specific lines, whether they are indicated by a specific line number, their contents, or first/last lines etc.

Let’s do an example that you will probably use someday – converting a FASTQ file to a FASTA file.  Recall from our earlier lectures that FASTQ files contain sequence data, just like FASTA files do, but with more information – the quality line – and different headers.  We can use sed to reformat a FASTQ file so that it is a FASTA in just one line.

First, let’s obtain a FASTQ file.  There’s a small sample FASTQ in the COURSE RESOURCES; you can transfer it to your linux system just like you did the sample.vcf.

If you like, use one of the tools you’ve learned above to take a look at the file and make sure it looks okay.  Once you are done, try this command:

﻿sed -n '1~4p;2~4p' sample.fastq

In this command we used sed to print the 1st and 2nd line out of every group of 4 (1~4 is the first of every 4 and when combined with the “;” to 2~4 we also get the second line; the “p” tells sed to print them to the console).  The “-n” flag tells sed not to print the pattern line (if you leave it out you’ll get duplicates of each line).

The FASTQ file is starting to look more like a FASTA – but the headers are very different, so we need to change those too.  We can use the search and replace features of sed to do that:

sed 's/^@/>/g' sample.fastq

The search and replace syntax is pretty straight forward once you understand it.  The basic battern is this:

sed 's/<text to find>/<text to replace it with>/g' file.ext

Simplified you can think of this as:

sed 's///g' file.ext

where you put the text to find between the first two slashes, and the text to replace it with between the last two slashes.  The “s” tells sed you are performing a search/replace, and the “g” tells sed to make it global – in other words, search and replace through the entire document.

Our example above is a little more complex, however.  Instead of just searching for “@”, we are searching for “@” only at the beginning of a line.  That  is what the “^” symbol is telling sed – find “@” at the beginning of the line.  Why are we doing that?  It turns out that the “@” symbol is also used in the phred quality symbol map, so we have to make sure we don’t accidentally replace all of the “@” symbols in the quality lines.

So our example, then, tells sed to find “@” at the beginning of a line and replace it with “>”, and to do it globally.  Go ahead and run the command and see how the file looks now.

Our last step is to string those two commands together – the first one to remove the 3rd and 4th lines of each group of 4, and the second to change the header:

sed -n '1~4p;2~4p' sample.fastq | sed 's/^@/>/g'

Go ahead and run that – the FASTQ file now looks like a FASTA file!  The only thing left to do is to save this output as a fasta instead of viewing it on the screen, so we can use the “redirect” command we learned earlier:

sed -n '1~4p;2~4p' sample.fastq | sed 's/^@/>/g' > sample.fasta

Between grep, awk, and sed, you have a series of powerful text file searching and editing applications.  Given that the majority of bioinformatics data files are in fact text files, you will likely be spending a lot of time using these or similar tools.

If you’ve already learned some python, you will know that you can also do many of these sorts of manipulations in python – so why would we use the BASH terminal in linux to do so?  The answer is easy – often it’s convenient to create a pipeline of analyses, and the easiest way to do that is BASH.  For example, you may execute a python application, take the results of that and pipe them into another python application – or maybe a C or perl application.  Linux BASH is the glue that can be used to create these pipelines, and if you need to tweak files between applications (e.g. renaming headers or changing the format), BASH can make it easy to do in the same script that pipes everything together.

BASH SCRIPTING

While we’re talking about it, let’s write a quick BASH script using some of the commands we’ve learned above.  For example, we can write a script that converts FASTQ to FASTA – that way you won’t have to remember all of the commands or retype them every time.  We’ll use my favorite editor, nano, to write the script.  To begin, enter the following command:

nano fastq2fasta.sh

The file “fastq2fasta.sh” doesn’t exist, but nano is happy to create it for us if we tell it to.  We are using the extension “.sh” to remind us that this is a shell script.

Now inside your nano editor enter these lines:

#!/bin/bash

# This program will convert a FASTQ file to FASTA format.

# To run it simply type bash fastq2fasta.sh <filename>

﻿sed -n '1~4p;2~4p' $1 | sed 's/^@/>/g' > $2

After adding this to the editor, press CONTROL+X – this tells nano that you want to save the file.  It will ask you if you want to save the modified buffer (meaning file):

Press “y”, then RETURN to accept the default name to save the file.  Now let’s walk through what this does, and how we can use it.  

Lines that start with a “#” are comment lines – nothing is executed on those lines, they are just there so that you can leave yourself comments on what the program does.  Comments are very important to add, so that anyone using your script later will be able to figure out what it does.

The exception to this rule is the first line – even though it starts with a “#”, the addition of the “!” tells linux that this line indicates what application should be run when it is launched.  By telling linx “#!/bin/bash” we are telling it to use the BASH application, which is stored in the “/bin/bash” directory.

The last line is the actual conversion routine.  It looks just like the line we used above, with one exception – instead of naming the input file (sample.fastq), the script uses “$1”.  Notice that we also use something similar in the output file name – “$2”.  In BASH, if you run a program and add terms after the program name, those terms are stored as variables.  And, you guessed it, they are named in order – so if we run this program and list the input file and the output file names, it will give us exactly what we want.  Let’s try it now.  In the terminal window enter the following command:

bash fastq2fasta.sh sample.fastq new.fasta

Notice that we ran our BASH script by first typing “bash” then the filename.  Since the file wants two variables names (the input file and the output file) we added “sample.fastq” as our input, and “new.fasta” as our output (using a different name than when we converted this earlier so that we can make sure our script works.  If all went well, when you list the contents of the directory you should see the new.fasta file.  Take a peek at it with more to see what it looks like.

On most linux systems, you wouldn’t be able to execute your new script unless you had told linux that it was an executable file.  To do that you would type the following command:

chmod a+x fastq2fasta.sh

This uses the chmod, or change mode, command, to add execute privileges to everyone (that’s where the a+x comes in – all users, add execute privileges).  You can do that to your script if you like.

There are many other useful commands in linux that you’ll want to use – there are commands to sort data (sort), take a file and eliminate redundant entries (uniq), compare two files to see where they differ (diff and comm), even a command to get instructions on how to use commands (man).  Explore on your own – start by entering this command to see how to use sort:

man sort



--- SOURCE: Week 9 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week  9 Lecture – Clustering & Dimensionality Reduction

CLUSTERING

Just as Genomics is the study of an entire genome – meaning the sequence of the genome, including annotations such as gene products (identified both through wet-lab work and computationally), promoters, SNPs, etc., Proteomics is the analysis of the genome in terms of expressed products – the transcripts that are actually produced by the genome. This is significant in that while the genome represents potential expressed products, the proteome is in fact all of the actual proteins and RNAs (not all of which code for proteins, recall) that are made in any given tissue at any given time. This gives us a very powerful way of comparing tissues and conditions (e.g. control vs disease states) in real time. Unfortunately, a full proteomic analysis means looking at approximately 50-60 thousand products (if you are using RNAseq data); a bit fewer (20,000 total) if you are looking only at proteins. 

One of the more common types of experiment that generates proteomics data is a SAGE analysis (Serial Analysis of Gene Expression), which results in a snapshot of all of the expressed mRNA's isolated from the target tissues on a microarray. A microarray is a small "chip" or slide that contains many thousands of "spots", each of which represents a different gene product. By probing the chip with a complex of probes (the biology is fascinating and I will not go into it in depth here, but feel free to dig deeper on your own if you like) and scanning it to detect signals, the expression levels of every gene for a given tissue/condition can be determined. RNAseq is a more modern PCR-based next- generation sequencing approach that is starting to replace SAGE and other microarray- based analysis, but the end result is the same – thousands (up to 60 thousand!) of different expression levels for unique gene products. 

It's important to keep in mind that the expression level of each gene product will differ from tissue to tissue and time to time - the amount of information, therefore, is overwhelming – and we need ways of viewing and analyzing such large data sets effectively. The raw data is typically expressed in a matrix (e.g. an Excel spreadsheet), but looking at several rows (possibly hundreds) and 30-60 thousand columns isn't something we can do without computational assistance. 

There are several approaches to doing this, and the first one we're going to focus on this week is Clustering. Clustering is the process of grouping similar data points to reduce the overall complexity of the data. A simple approach would be to plot each data point in an N-dimensional space (where N may be as simple as 2 – gene and expression – but may be 3 if you are looking at a time series, or more if you are adding additional variables) and computing a distance matrix between each possible pair of points. Then, genes with very small distances can be clustered (grouped!) together, since they may be functionally related (e.g. co-regulated, expressed at the same times, at the same levels). 

Yes, this is a big assumption to make. In practice the more dimensions you add to the data, the more likely that this assumption is true. More advanced techniques like multivariate correlation analysis (Google "StickWRLD" for an example) may be able to more accurately predict related genes, but clustering does tend to provide useful data... 

Two key principles in clustering are homogeneity (the elements within a cluster should be close to one another) and separation (elements in different clusters, and clusters themselves, should be further apart from each other). 

For example – given these data points, the clusters should be easy to identify, as both the homogeneity and separation are good: 

From these data you would (probably) decide that these are the clusters: 

And you'd be correct! 

There are essentially three different strategies when it comes to clustering: Agglomerative, Divisive, and Hierarchical: 

Agglomerative: Starts with every element in it's own cluster and iteratively joins clusters together. 

Divisive: Starts with all data in one big cluster and iteratively divides it into smaller clusters 

Hierarchical: Organizes elements into a tree where the leaves represent genes and the length of the paths between leaves is proportional to the distances (not sequence distances but based on the N-dimensional plot!) between them. Similar genes lie within the same subtree. 

Of these, Hierarchical clustering is the most commonly used. Hierarchical clustering is similar to the techniques we use for computing evolutionary histories: 

Start with n clusters, where n is equal to the total number of elements (data points). At this point each OTU (operational taxonomic unit) is it's own cluster. 

Compute the distance between all clusters. 

While there are more than one cluster, find the two closest clusters and merge 

them into a new cluster containing all of the elements of the two. 

Repeat steps 2 and 3 

Obviously you do not want to end up with one large cluster, which is the logical endpoint of a hierarchical clustering – at some point you have to stop adding clusters to one another. This threshold is typically a default. 

Hierarchical clustering is very computationally expensive even with relatively small data sets, and if a data point is misclustered early on, the error will remain and not be corrected. 

Another common method of clustering is K-Means clustering. In K-Means clustering, the data is split in k random clusters. Then the centroid (the center of a given cluster) is computed for each cluster. Then each data point is assigned to the closest centroid and the process is repeated until data cannot be reassigned better than it already has been. Let's start with a simple example where K is set at 2: 

The red and blue "x" symbols are the initial centroids if you divide the data along the diagonal: 

Now recomputed the cluster centers based on this division: 

The "x"s are hard to see but they are there – now let's reassign data points based on proximity to these new centroids – notice that colors change to reflect the new assignment: 

As you can see even with an arbitrary centroid placement you can rapidly begin to correctly cluster the data points! This example only displays two iterations; in the real world you may repeat this as needed until there are no changes. 

This iterative processing ensures that you do not get trapped in a localized optimum, and is very memory and CPU efficient – distinct advantages over hierarchical clustering! The disadvantage to K-means clustering is that it is very sensitive to the initial parameters – if the centroid initialization is done poorly the clustering may take many more iterations, and may even result in a poor clustering. To compensate for this, it's best to use several different initializations and chose the best outcome. 

DIMENSIONALITY REDUCTION

While clustering takes all of the data points and groups them based on some common criteria, there is another way to deal with very large data sets:  dimensionality reduction.  Dimensionality reduction is the process of transforming data from a higher-dimensional space to a lower-dimensional space, while retaining the meaningful properties of the original data.  One way to think of this is to picture a data set that might be meaningful when plotted in 3 dimensions or more – how can we represent this in a simple two-dimensional graph?  By reducing the overall dimensions of a dataset we make the data more tractable both to computational analysis and to human visual analysis.

While there are many different ways to reduce dimensionality, we are going to focus on two techniques that apply a linear transformation to the original data:  PCA and LDA.

PCA:  PRINCIPLE COMPONENTS ANALYSIS

PCA is an unsupervised technique that ignores any class labels that may be assigned to the data and finds the “direction” or vector of maximum variance – identifying the “principle components” (the axes of the largest variance) of the data.  Essentially PCA gives you a number of axes where the first axis is responsible for the largest amount of variance, the second axis is responsible for the next largest amount, and so on.  By reducing the data down in this fashion, the first two (or three) axes can be plotted, thus representing the majority of the variance.  As an additional benefit, this sort of data reduction can be used prior to training a neural network (we’ll learn more about that later when we talk about machine learning) to make the resulting model more tractable computationally.

There are six basic steps to performing a PCA analysis:

Compute the mean vector (µ) for your data

Subtract mean from the given data.

Calculate the covariance matrix.

Calculate the eigen vectors and eigen values of the covariance matrix.

Choose components and form a feature vector.

Derive the new data set.

Let’s walk through a simple example of a dataset with just two variables, x1 and x2, and 12 data points.  Recall that in “real world” examples, you would be dealing with multiple variables, and the actual mathematical analysis would involve many more dimensions than are shown below:

The largest amount of variance in the display above can be represented by drawing a line that splits the overall lengthwise through the largest dimension:

To reduce this two-dimensional dataset to a one-dimensional dataset (the simplest example, again), we would collapse the data onto that line, which is the first principal component:

Which reduces to this:

We have lost some of the original information through this reduction, but we have retained the most important axis of variance.

Now let’s capture the second principal component from this same data set.  Ideally this will be orthogonal to the first component, doing our best to capture the variance in the data that was not captured by the first component.  So in our simple two-dimensional example, that would look like this:

As you can see, the two principal components are perpendicular to each other, and capture independent elements of the dataset.  To do the full transform we would collapse both and create the new data set.

Since this is already a two-dimensional data set, performing a PCA and projecting the original dataset onto the first two principal components would not result in a loss of data (since we are transforming from a two-dimensional dataset to a new two-dimensional dataset).  Instead, we would be merely rotating the data to use new dimensions.  You get the idea, however – with large multidimensional (multivariate) data, you can reduce multiple starting dimensions (which may be equal to the total number of variables) down to their principle components and plot whatever number of axes is convenient (e.g. two or three).  In general, the data will tend to follow an 80/20 rule, meaning that the majority of the variance will be explained by a very small number of principal components.  When you actually perform a PCA analysis, the software will tell you how much variance is explained by each component – and the number of Principal Components will always be smaller than the number of attributes/variables in the original data set.

LDA:  LINEAR DISCRIMINANT ANALYSIS

While PCA is unsupervised and ignores class labels, LDA instead explicitly uses the class labels to attempt to model the data.  This requires that the categorical groupings are known for each point in the dataset.  LDA analysis then uses those classifications to attempt to identify the variables most responsible for the variance in the data.  For example, my lab had a series of patients (n~100) from which we isolated proteins which were identified by mass spectroscopy.  We could classify each patient to a group (e.g. Control, Severe disease, Mild disease).  The LDA analysis we performed used those groupings to look for the proteins that most strongly contributed to the variance seen between the groups.  This is similar to a PCA in terms of variance detection, but dissimilar in that the groupings are assigned prior to analysis – which is what makes LDA a supervised form of analysis.  Like PCA, the LDA outputs axes which can then be plotted, reducing the dimensionality of the data to a human readable form.  There is a drawback to LDA, however, in that the number of resulting axes will always be N-1, where N is the original number of dimensions in the dataset.  So if you are looking at a bimodal dataset (e.g. Malignant vs Benign cancer tissue), LDA will produce a value, but not something that is plottable the way a PCA analysis is.  With greater than 2 dimensions, however, LDA can be a very powerful additional analysis to include.



--- SOURCE: Week 10 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week 10 Lecture – KMERS

In bioinformatics, a k-mer is a sequence of length k, taken from a larger fragment of sequence data.  Any sequence can be broken into a number of k-mers, where the number of k-mers is dependent on both the size of the original sequence and the size of k.  For example, given the sequence ACTGATAT, what k-mers of size 3 can we identify?  We essentially use a window of size three and move from the first position to the second etc.:

ACT

CTG

TGA

GAT

ATA

TAT

Notice that these are all unique, which won’t always be the case – with a larger sequence you’ll inevitably find that certain kmers will occur more than once.  For example, if our sequence instead was:

ACTTATAT:

ACT

CTT

TTA

TAT

ATA

TAT

The 4th and 6th kmers are identical for this sequence.  So if we were to create a table with a kmer count, we would have:

For the sequence ACTTATAT

You can imagine that as the sequence you are examining gets larger, the number of kmers will increase – both the unique kmers (although there is a logical limit to this, as there are only so many combinations of four bases that are possible) as well as the number of each unique kmer.

So – now that we can count kmers, what can we do with them?  It turns out that kmer frequencies can be extremely useful for bioinformatics!  If we plot a graph of kmer count vs kmer distribution (which is called the kmer spectrum), we can see that most species have a unimodal distribution – meaning that there will be a single peak to the plot:

Figure 1:  Adapted from Wikipedia, https://en.wikipedia.org/wiki/K-mer#In_Bioinformatics_Pipelines

This is not the case for mammals, interestingly enough, which all exhibit a multimodal distribution – but even within those genomes there can be specific regions that exhibit a unimodal model.  Note that the spectrum may differ with different values of k, but if we take that into account we can use kmer frequencies to establish a type of unique signature, or fingerprint, for each species.

Let’s take a look at some real-world applications of Kmer analysis in bioinformatics:

GENOME ASSEMBLY VIA KMERS

In next-gen sequencing (e.g. Illumina short-read technology), the read length is typically around 100bp (which is equivalent, then, to a kmer of size 100, or a 100-mer).  In a shotgun sequencing reaction, where we are trying to reconstruct the genome of an organism using these short reads, only a small percentage of the 100-mers that actually occur in the genome will be reproduced through the sequencing process – largely due to coverage gaps and sequencing errors that are inherent in the technology.  This means that a De Bruijn graph used to assembly the genome from the reads cannot be constructed (since one of the caveats of De Bruijn graph building is that all possible kmers must be present in order for the assembly to take place via kmer overlap).  

The solution is to take the sequence reads (the 100-mers) and break them into smaller kmers – kmers of a size (say, 4) such that all possible kmers present in the genome of that size will be represented in the new kmers derived from the sequence data:

Figure 2:  Adapted from:  https://en.wikipedia.org/wiki/K-mer#/media/File:K-mer-example.png

After breaking the 100-mers into 4-mers, any 4mers that are present more than once are “uniqued” so that only one instance of them is present in the resulting 4mer set.  Then the se 4mers can be assembled into a contiguous sequence.

Not only does this approach make rapid genome assembly possible despite the errors inherent in the sequence technology, the process is greatly simplified computationally – processing millions of 4mers is still less computational intense than processing tens of thousands of 100mers.  An additional advantage is that reads which have a kmer spectrum that does not conform to the expected spectrum for whichever species you are sequencing can be excluded as they are most likely the result of contaminants in the sequencing reaction (e.g. if you are sequencing a human genome and the kmer spectrum identifies E. coli reads, you will want to exclude those from the human assembly).

So far we’ve talked about 3mers, 4mers, and 100mers – how does one choose the right value for k when assembling sequence reads into a genome?  The value of k has significant effects on the resulting sequence assembly, and those effects are different for large values of k vs small values:

Lower values of k:

Advantages:

Decrease number of edges in the De Bruin graph, which means less memory required for storage 

Increase probability of overlap of kmers for De Bruijn graph creation

Disadvantages:

Increases the number of vertices which can point at a single kmer, increasing the potential number of paths to traverse (thus more computationally intensive)

The amount of information per kmer is decreased

Decrease the ability to deal with microsatellite or repeat regions

Larger values of k:

Advantages:

Fewer vertices in the De Bruijn graph, which means fewer paths to traverse, so computationally faster and less intensive

Larger values of k make it easier to deal with regions containing small repeats

Disadvantages:

Increased number of edges in the De Bruijn graph, which means more memory required for storage

Larger values of k increase the risk that the kmers may not overlap, resulting in more small contigs rather than a complete assembly

SPECIES IDENTIFICATION VIA KMERS (including contaminant detection)

Above we mentioned that you can use the kmer spectrum to identify species.  If you were working from a pure culture and simply trying to identify it, you’d be far better off using the 16S rRNA (or 18S if the organism is eukaryotic).  When you are working with an environmental sample, however, or any other mixed population, it may be easier/faster to take shotgun sequence data (rather than PCR amplify and sequence an rRNA) and apply a kmer-based identification approach such as Kraken.  According to the Kraken github page, Kraken uses “exact k-mer matches to achieve high accuracy and fast classification speeds. This classifier matches each k-mer within a query sequence to the lowest common ancestor (LCA) of all genomes containing the given k-mer. The k-mer assignments inform the classification algorithm.”  Essentially this means that Kraken requires a custom kmer database that is trained on existing sequence data from NCBI – a large subset of sequence data is used to develop kmer tables for a variety of values of k and then stored as the custom database relating kmer spectra to species.  In the “old” days (2019), you would have to construct this database yourself, which meant a few days of computer time, downloading chunks of NCBI and then doing kmer counting.  Today you can download the Kraken database from an AWS (Amazon Web Services) site maintained by the software maintainer, which makes setting it up far easier.

One of the most interesting use cases of Kraken is that of full-metagenome reassembly.  While early metagenome studies relied exclusively on 16S sequences, it is becoming more common to combine that approach with a metagenome “genome recovery” approach.  In this type of study, DNA is isolated from an environmental sample (which might be a soil or water sample, or a sample from a patient, e.g. a buccal swab or stool sample) and subjected to shotgun sequencing.  The resulting reads will sequence data from every species present in that sample – conceivably many thousands.  Those reads are first sorted into bins using Kraken, where each bin represents a unique species.  Reads which cannot be uniquely sorted into a bin are placed in a separate bin for unidentified reads.  Then, each bin is treated as a separate genome assembly project and a “complete” (or as complete as possible given the number and quality of reads) is created.  There are many additional steps in this process, including QA/QC at many of the stages – here’s an example workflow from a software pipeline called “metawrap” that automates as much of this as possible:

Figure 3.  Metawrap Workflow (from https://github.com/bxlab/metaWRAP)

This process of genome recovery has a very different application than the “traditional” metagenome approach, in which the purpose of the study is to classify (identify) the species (or more likely genus given the limitations of 16S) present in the sample and determine their relative abundance.  With the genome recovery approach, instead of identifying hundreds or thousands of species, the genomes of the most prevalent species are reconstructed so that their metabolic potential can be analyzed.  For example, if the most prevalent species have never been cultivated (which applies to many bacterial species), nothing is known about their metabolism or metabolic byproducts.  By reconstructing the genome of these non-cultivatable species, we can do comparative genomics and identify which genes are present based on comparison to known genes.  This then can be imputed to help determine the metabolic profile of the organisms using software that predicts metabolite data from sequence data, such as MelonnPan.  

The limitation is that not all of the species will be present in sufficient quantities for this sort of approach to fully recreate more than the top dozen or so most prevalent species.  That being said, recreating the metabolics of the top twelve most prevalent species, some of which may be uncultivable, is exactly the sort of imputation that bioinformatics is best at.



--- SOURCE: Week 11 Lecture.docx ---

BIFS 614 – Data Structures and Algorithms

Week 11 Lecture – Machine Learning in Bioinformatics

Since the early 1950’s we have been anticipating the development of computer-based “artificial intelligence”, or AI.  While we haven’t gotten to the point of “Hal 9000” or “SkyNet” yet (thankfully!), we have developed many ways in which machine learning, or ML, can be used to help us analyze data.  So what, exactly, is ML?

ML is, essentially, a system for creating statistical models (referred to as neural networks) that improve “automatically” through experience, or training.  There are many different kinds of ML, but the simplest form is an Artificial Neural Network, or ANN.  An ANN is a model based on a series of connected nodes called artificial neurons, which are analogous to neurons in a biological brain.  Each node (artificial neuron) can transmit information to other nodes, and when nodes receive information, they process it and signal additional nodes that they are connected to.  Whether or not data gets transmitted to a node depends on statistical weights and thresholds; the output of any given neuron is computed by a non-linear function representing the sum of its inputs.  

The signal sent by an ANN node will be a real number, rather than a neurotransmitter, but you can see why we can it a “neural network”.  The connections between the nodes (neurons) are called “edges” (referring graph theory), and each edge will have a weight that is adjusted as the entire network is trained – as it learns.  This weight modifies the strength of the signal, and typically a threshold is set at each edge so that the signal only goes through if the aggregate signal that a node (neuron) receives exceeds that threshold.  So the job of the neural network training is to adjust all the edge weights until the model that the ANN represents accurately describes the data.

For any ANN, there’s always an INPUT layer (where the data comes in), and OUTPUT layer (where the “answer”, or classification) is output, and one or more “hidden” layers:

Figure 1:  Typical ANN structure (from https://en.wikipedia.org/wiki/Machine_learning)

These different layers (with exception of the OUTPUT layer) all perform different mathematical transformations to their inputs from the previous layer.  It’s a bit of an art, deciding how many hidden layers to use – you have to have one, but there’s no logical limit (although computationally the CPU demands rise as you add layers).  Most people that build ANNs use a “trial and error” approach, using different numbers of hidden layers and other configuration variables and keep the final model that best is able to predict the data.

An aside:  how do we know the predictions an ANN makes are correct?  Typically we take a large dataset – the larger the better, because more data typically means a well-trained neural network – and we train the ANN with 80% of the data (bear in mind that for this dataset we know what the answers should be).  Then, once we’ve trained an ANN, we test it using the remaining 20% of the data, without giving the ANN the answers.  If the ANN is good at predicting the answer (say, 90% accuracy or better), the ANN can be applied to new data where we do not yet have the answer.

We aren't going to do a deep dive into the mathematics behind ANNs – as a bioinformaticist you are far more likely to build a neural network using existing tools and customizing the layers to your data than you are to code a new form of NN.  You should, however, be aware that there are many different types of NN, with different ideologies underlying their approach – be sure to read this week's OERs!

ML in BIOINFORMATICS

Let’s take a look at some real-world examples:

Researchers from the Harvard Medical School took a dataset containing 64 images of cells from normal, healthy human female breast tissue as well as 112 images of cells from patients with breast cancer.  The images were used to train and test two ANN ML algorithm containing a single hidden layer (after some basic image processing).  The neural networks were between 59% and 70.49% accurate at predicting which tissues represented breast cancer.  While this doesn’t sound very high, when you take into account that clinical diagnosis accuracy is around 50%, the ANN performed very well.  (Kaymak, Helwan, and Uzun, Procedia Computer Science 120 (2017) 126–131).

Google Health (yes, a division of that Google) similarly developed ANNs used to diagnose both breast and lung cancers, with similar results.  Similarly, ANNs are being used to help develop better drugs, medicines, and even foods.

ANNs aren’t by any stretch of the imagination “thinking machines” – the original goal of the ANN was to solve problems in a fashion analogous to the human brain.  As it they developed it was found that they were very bad at generalized problem solving, but very good at performing very specific tasks – like examining images to see if a patient has cancer.

YOUR FIRST NEURAL NETWORK

Enough talk – let’s go ahead and build a neural network.  We’ll be building this example in R, which after Python is probably the most popular way to construct ANNs.

First you’ll need a functioning R installation.  If you don’t already have R on your computer, you can download it from here:  https://mirror.las.iastate.edu/CRAN/.  Once you have it downloaded, you can install it as you would any other application. 

You will also need RStudio, which is an excellent IDE (integrated development environment) for R.  If you don’t already have RStudio, you can download it from here:  https://rstudio.com/products/rstudio/download/#download.  

Once you have these programs installed, go ahead and launch RStudio.  You should see something like this:

The next thing we have to do is to install some packages.  R is a very versatile – and extensible – application; chances are that for any specific function that you want to run, someone has already written a package that will help you.  For this exercise we’ll install the caret package, which has the appropriate ANN programs.

To install the caret package in R, make sure your cursor is in the CONSOLE tab on the left and enter this command:

install.packages(“caret”)

The last time I ran this, R asked me if I wanted to install additional packages required by caret from source.  If you get asked this question, just enter “Yes” and let R continue – it may take a few minutes:

Once R finishes, you are ready to write the R script.  From the FILE menu, choose “New File”, then “R Script”:

You’ll see that the left panel now includes an untitled tab at the top – this is where your R script will be written:

Start by adding a “header” line:

# This is an R script designed to create a neural network

Next we’ll make sure that if the script gets used on a different machine that the proper libraries are installed with the install.packages commands:

# Install needed packages

install.packages("caret")

install.packages("e1071")

install.packages("kernlab")

install.packages("randomForest")

Lastly we can tell R to load the caret library:

# Load the necessary libraries:

library(caret)

Notice that there is a “run” button at the top of this window:

This button lets you execute the script one line at a time.  So, for example, if you put your cursor back at line 1 and hit the RUN button, it’ll execute the next code line, which is line 4.  You don’t have to execute your program one line at a time, but for trouble-shooting it makes things much easier.  Go ahead and run all of the lines you have added so far.

So now we’ve loaded caret (as well as the libraries it depends on, lattice and ggplot2), let’s continue.  We’ll use one of the built-in tutorial datasets, which as it happens is a biological example based on irises (the flowers).  Add and run the following lines:

# Load the test dataset

data(iris)

# rename the dataset

dataset <- iris

When you run these lines, you’ll see that the top right window (the ENVIRONMENT window) changes, showing you the variables that have been created.  You can click on them to view them, and if they get changed or modified, you have easy access to see what happened.  This is why programming with an IDE is so much better than the “old fashioned” way!

If you do click on either one (they’re the same, we just renamed the iris data to dataset) you’ll see that this is a table of data on petal and sepal length and width.  The data also indicates which species had which values.  Sounds like a perfect training set to create a neural network that can identify a species based on the petal and sepal size, doesn’t it?  To return to your script you can use the tabs at the top left; “dataset” is the one you just opened, “untitled1” is your script.

Speaking of which – let’s save our progress so far.  Use the FILE -> SAVE menu option to save your script to your desktop.  I called mine “ANN.R” but you can call yours anything you like.

Now that we have the data, let’s break it into training and testing groups.  Recall that we train our ANN with part of a known data set, and then test it with the other part.  It’s a rule of thumb to use 80% to train and 20% to test, but your mileage will vary extensively depending on how much data you have.  More data is always better when training a neural network!  Go ahead and add these lines:

# create a list of 80% of the rows in the dataset 

index <- createDataPartition(dataset$Species, p=0.80, list=FALSE)

# select 20% of the data for validation

validation <- dataset[-index,]

# use the remaining 80% of data to train the network

training <- dataset[index,]

Let’s walk through these one line at a time.  

The first command creates an index of the rows by random sampling.  If you run the line and then click on the index variable in the ENVIRONMENT tab, you’ll see what I mean – it’s just a list of row numbers.  The variables also tell you how many members there are, so the full dataset has “150 observations”, while our index only contains 120 entries – which is 80%. 

The second line uses the index we created and selects all the lines that do NOT match those row ID’s – that’s done by the minus sign in the “dataset[-index,] command.  So the 20% of the dataset ends up in the “validation” variable.  Click on it to see!

NOTE:  this is an exercise in ANN construction rather than a full R tutorial.  If you are not familiar with R, you should consider taking a course in it – it’s extremely useful for statistical analysis, and it’s probably the absolutely best graphing application there is, thanks to the ggplot2 library that we’ll be using!

The third command does the reverse of what the second line did – it adds the 80% of the rows in the index to the “training” variable.

After running these three lines your ENVIRONMENT tab on the top right should look like this:

DATA EXAMINATION

Before we move on, it is useful to collect some basic information about the dataset.  Add the following lines to your script and run them:

# summarize the class distribution

percentage <- prop.table(table(training$Species)) * 100

cbind(freq=table(training$Species), percentage=percentage)

The summary of the class distribution outputs this into the Console on the bottom left:

> # summarize the class distribution

> percentage <- prop.table(table(training$Species)) * 100

> cbind(freq=table(training$Species), percentage=percentage)

           freq percentage

setosa       40   33.33333

versicolor   40   33.33333

virginica    40   33.33333

This tells us that the three different species are equally represented in the training data.  Now let’s do a basic summary of the actual measurements with these lines:

# summarize attribute distributions

summary(training)

The output to the console for this command is:

> summary(training)

  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width          Species  

 Min.   :4.300   Min.   :2.000   Min.   :1.100   Min.   :0.100   setosa    :40  

 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300   versicolor:40  

 Median :5.800   Median :3.000   Median :4.250   Median :1.300   virginica :40  

 Mean   :5.814   Mean   :3.038   Mean   :3.753   Mean   :1.189                  

 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800                  

 Max.   :7.700   Max.   :4.100   Max.   :6.900   Max.   :2.500     

It might be easier if we visualize the data, however, so that we can get an idea of the differences between species.  Let’s try constructing some univariate plots.  Add the following lines to your script and run them:

# Visualize Dataset:  Univariate Plots

# split input and output

x <- training[,1:4]

y <- training[,5]

# boxplot for each attribute on one image

par(mfrow=c(1,4))

for(i in 1:4) {

  boxplot(x[,i], main=names(iris)[i])

}

In the first set of commands we split the training data into the numeric data (the sepal and petal measurements) to use as our “X”, and the classification as our “Y”.  The next set of commands generates a single graph with four whisker boxplots.  It’s much easier to see the distribution of the data on this plot!

Let’s repeat our plots, this time separating the species so that we can see the distribution of variables for each species:

# box and whisker plots for each attribute

featurePlot(x=x, y=y, plot="box")

When you run these lines, you see a new plot generated:

From this plot you can see that there is overlap in both the sepal length and sepal width between species, but the petal length and width seems to be clearly distinct between the three.  This kind of separation suggests that the data will be a good fit for training and using an ANN.

Now let’s define some parameters for our ANN.  We’ll take the training data and subdivide it one more time, into 10 sets.  We’ll test out a few different ANN algorithms with this data, running each test 3 times to get a consensus of which algorithm we should use for the “real” ANN.  Note that you don’t have to do this – you could just use the full training data and test the algorithms – but this acts almost like a bootstrap process, checking the training data for sensitivity at certain rows.

# Set Parameters:

# Run algorithms using 10-fold cross validation

control <- trainControl(method="cv", number=10)

metric <- "Accuracy"

Once you have entered and run these lines, our test parameters are ready to go.  The “Accuracy” metric is the variable we will use to evaluate each test.  You don’t have to use a variable for this, but it makes it easier to change for multiple tests if we use a variable rather than hard-coding it into each test.

MODEL TESTING

Now let’s go ahead and test four different ANN methods: 

Linear Discriminant Analysis (LDA)

k-Nearest Neighbors (kNN)

Support Vector Machines (SVM)

Random Forest (RF)

These methods differ in their nature of the modelling engine – for example, the LDA method uses a  simple linear model, SVM and RF use complex linear models, and kNN uses a nonlinear model.  While each of these methods should work, it’s likely that one of them will be slightly better in terms of predictive accuracy – which is why we’re testing them all.  Go ahead and add these lines to your script and run them:

# Test our 4 different methods:

# a) linear algorithm:  LDA

set.seed(7)

fit.lda <- train(Species~., data=dataset, method="lda", metric=metric, trControl=control)

# b) nonlinear algorithm: kNN

set.seed(7)

fit.knn <- train(Species~., data=dataset, method="knn", metric=metric, trControl=control)

# c) complex linear algorithms:

# SVM

set.seed(7)

fit.svm <- train(Species~., data=dataset, method="svmRadial", metric=metric, trControl=control)

# Random Forest

set.seed(7)

fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control)

You should now see that there are four different “fit” variables in the ENVIRONMENT tab, one for each of the networks.  Each of those contains the accuracy – let’s check it out with this command:

# summarize accuracy of models

results <- resamples(list(lda=fit.lda, knn=fit.knn, svm=fit.svm, rf=fit.rf))

summary(results)

The first line generates a table of the results, and the second line summarizes them to the console.  The line we are looking for most specifically is the “Accuracy” line, which will let us know which of our methods performed best:

> summary(results)

Call:

summary.resamples(object = results)

Models: lda, knn, svm, rf 

Number of resamples: 10 

Accuracy 

         Min.   1st Qu.    Median      Mean 3rd Qu. Max. NA's

lda 0.9333333 0.9500000 1.0000000 0.9800000       1    1    0

knn 0.8666667 0.9333333 1.0000000 0.9666667       1    1    0

svm 0.8000000 0.9333333 0.9666667 0.9466667       1    1    0

rf  0.8666667 0.9333333 0.9666667 0.9600000       1    1    0

Kappa 

    Min. 1st Qu. Median Mean 3rd Qu. Max. NA's

lda  0.9   0.925   1.00 0.97       1    1    0

knn  0.8   0.900   1.00 0.95       1    1    0

svm  0.7   0.900   0.95 0.92       1    1    0

rf   0.8   0.900   0.95 0.94       1    1    0

From this it’s pretty clear that the linear approach of the LDA was best able to model our data, with a mean accuracy of 98% - pretty good considering how few data points we used!  That isn’t to say that the other methods didn’t also do well – anything above 90% is acceptable, and all four methods were 94% or higher.

The real test, of course, is to take our test data and see how well the model predicts the species for the 20% of the original dataset that we excluded from the training data.  Since we already have the model constructed (it’s in fit.lda), we can run the predict command:

# estimate skill of LDA on the validation dataset

predictions <- predict(fit.lda, validation)

This stores the predictions for the validation data in the predictions variable.  Let’s print out the matrix and see how we did:

confusionMatrix(predictions, validation$Species)

Confusion Matrix and Statistics

            Reference

Prediction   setosa versicolor virginica

  setosa         10          0         0

  versicolor      0          9         1

  virginica       0          1         9

Overall Statistics

               Accuracy : 0.9333          

                 95% CI : (0.7793, 0.9918)

    No Information Rate : 0.3333          

    P-Value [Acc > NIR] : 8.747e-12       

                  Kappa : 0.9             

 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: setosa Class: versicolor Class: virginica

Sensitivity                 1.0000            0.9000           0.9000

Specificity                 1.0000            0.9500           0.9500

Pos Pred Value              1.0000            0.9000           0.9000

Neg Pred Value              1.0000            0.9500           0.9500

Prevalence                  0.3333            0.3333           0.3333

Detection Rate              0.3333            0.3000           0.3000

Detection Prevalence        0.3333            0.3333           0.3333

Balanced Accuracy           1.0000            0.9250           0.9250

Our accuracy is 93% - still pretty good!  

Now, how would we go about getting an answer from just one piece of data?  We can use the predict function with the fit.lda model and just feed it values.  For example, if we wanted to give the model just one value from the validation data we could do this:

# Get a prediction on just one (row 2 in validation data)

predict(fit.lda, validation[2,])

This sends just row 2’s data and tells us that this is from the species setosa.  We’re still using existing data, however – let’s make up some new data, say, something one of our researchers acquired from the field, and test *that*.  To do that we’ll have to create a new data frame containing the data:

field.data <- data.frame("Sepal.Length"=7, "Sepal.Width"=5.1, "Petal.Length"=3.2, "Petal.Width"=0.7)

Now we run the prediction on this new data:

predict(fit.lda, field.data)

The answer prints out in the console – this field sample is from species setosa!

Note:  this exercise was adapted from:

https://machinelearningmastery.com/machine-learning-in-r-step-by-step/