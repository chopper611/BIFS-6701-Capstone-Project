--- SOURCE: BIFS 614 HW1.docx ---

BIFS 614 – Data Structures & Algorithms

Homework 1:   Algorithms & Data Structures

INSTRUCTIONS TO INSTRUCTOR

Choose any combination of these questions that add up to 100 points.  Change the questions every term to help cut down on cheating via internet.  Feel free to add additional questions; if you do please forward them to robert.rumpf@umgc.edu for inclusion in the master list.

Q (5 pts):  Write the line of code that would install the package ggplot2 in R.

Q (5 pts):  Write the line of code that defines the variable sequence as AATCGA in python.

Q (5 pts):  What is a CIGAR string?  How is it generated, and where will you find it?

Q (5 pts):  What is a BAI file?

Q (10 pts):  Compare and contrast VCF and GFF files.

Q (10 pts):  Compare & Contrast FASTA and FASTQ files.

Q (10 pts):  What are the required fields for a VCF file?

Q (10 pts):  What is BigO?

Q (10 pts):  What are comment lines in an algorithm?  When should they be used?

Q (15 pts):  Describe the differences between TSV, CSV, and Excel.

Q (15 pts):  What are SAM and BAM files?  What are the differences between them files?  How are they generated?

Q (25 pts):  Convert the sample.fastq at https://culture-bioinformatics.org/UMGC/ to a fasta file and include it as an attachment to this homework.  Explain how you did this and what the differences are.  Was any information lost in the conversion?

Q (25 pts):  For a given algorithm, how will BigO change if the algorithm is run on different hardware (e.g. a faster processor)?  How will it change if a larger data set is run through the algorithm?  Explain.

Q (25 pts):  Using pseudocode, describe an algorithm that sums the values between 1 and N for a given value of N.  What do you think the BigO will be for this algorithm and why?

Q (25 pts):  Using pseudocode, describe an algorithm that searches for all instances of “ATC” in a given sequence.  What do you think the BigO will be for this algorithm and why?

Q (25 pts):  Using pseudocode, describe an algorithm that searches for palindromes of size 5 bases in a given sequence.  What do you think the BigO will be for this algorithm and why?



--- SOURCE: BIFS 614 HW2.docx ---

BIFS 614 – Data Structures & Algorithms

Homework 2:   Indexing and Searching

INSTRUCTIONS TO INSTRUCTOR

Choose any combination of these questions that add up to 100 points.  Change the questions every term to help cut down on cheating via internet, including changing the sequences and or other variables.  Feel free to add additional questions; if you do please forward them to robert.rumpf@umgc.edu for inclusion in the master list.

Q (10 pts):  What is a relational database and why is it useful in bioinformatics?

Q (10 pts):  Define kmer.  Provide 3 separate examples.

Q (10 pts):  Define a heuristic algorithm.

Q (10 pts):  If you have 25 taxa, how many possible rooted trees can you construct?

Q (20 pts):  Given the sequence AATTACAGGCGACAGATA find all kmers of size 3.  Create a table that displays their counts and positions.

Q (20 pts):  Construct a keyword tree for these three sequences:

AATACCAGAGCGAGCTTAGACG

AATACCGGACTAACGAATGATT 

TCCAGACGACGTTTAAGCGACT

Q (20 pts):  Create a suffix tree for these three sequences:

AATACCAGAGCGAGCTTAGACG

AATACCGGACTAACGAATGATT 

TCCAGACGACGTTTAAGCGACT

Q (20 pts):  Describe how a hash function works, using both descriptive text and a diagram.

Q (20 pts):  Describe how BLAST uses hash tables to speed up sequence searching.



--- SOURCE: BIFS 614 HW3.docx ---

BIFS 614 – Data Structures & Algorithms

Homework 3:  Data Parsing

INSTRUCTIONS 

You have one week to complete this homework.  As always you must show all of your work (including LLM prompts and corrections as well as all outputs) for full credit. 

Q1 (10 pts):  Describe grep and what it can be used for.  Be sure to describe what the following flags do:  -v, -F, -r

Q (10 pts):  Describe the function of the “|” operator and when you would use it.

Q (10 pts):  Describe sed and what it can be used for.  Be sure to include an example.

Q (15 pts):  Describe awk and what it can be used for.  Be sure to include an example and describe what the following flags do:  -F, &&, and !.

Q (15 pts):  Write the full linux command you would use to convert string 1 below to string 2:

String1:  gene1gene2gene3

String2:  gene1gene2gene3gene4

Q (15 pts):  Write the linux command you would use to combine 3 text files together into 1 large text file.  Explain what each step does.

Q (15 pts):  Write the linux command you would use to convert a FASTQ file to a FASTA file.  Explain what each step does.



--- SOURCE: BIFS 614 HW4.docx ---

BIFS 614 – Data Structures & Algorithms

Homework 4:  SQL and MariaDB

For this homework you will be installing the MariaDB database application on your linux installation, creating a database and some tables.  In order to receive full credit for your work, you must show all commands used and all outputs.

10 pts:  Install MariaDB on your VirtualBox Linux system using the command line.  Use the following three commands to do so.  Each command must be entered as a SINGLE LINE in ubuntu.  Submit a screen capture of your terminal window after each step for full credit:

sudo apt update

sudo apt install mariadb-server

sudo mysql_secure_installation

NOTE:  after the third command you will be asked to enter the current password for root.  You should use “ubuntu” (without the quotes).  Then proceed to answer Y to all of the following questions.

10 pts:  Test whether or not your MariaDB server is up and running by entering the following command.  Submit a screen capture of the resulting output and press “q” to return to the command prompt:

sudo systemctl status mariadb

10 pts:  Log in to your SQL database.  As before submit a screen capture for full credit:

sudo mysql

10 pts:  At the MariaDB> prompt, create a new database using the following command, using your first name as the name of your database (I’ll us my name in the example):

create database WOLFGANG;

Notice the semicolon at the end of the command.  All SQL commands end with a semicolon.  Now type this command to list all of the existing databases:

show databases;

Notice that MariaDB (as any SQL database) has created other databases as support structure.  As before submit a screen capture of the resulting output.

 Now let’s create a user for the new database using the following command (again, I’m using the database WOLFGANG; you should use the one you created above, and use your name for the user, not mine):

﻿create user 'wolfgang'@localhost identified by 'mypassword';

	Let’s make sure the user was created by entering this command:

select user from mysql.user;

This command looks at the mysql database (which you’ll recall was listed when you listed the databases) and specifically shows the contents of the table user – which is why we used mysql.user above; it specifies database.table.

MariaDB now has a user, but that user has no privileges.  We need to specifically tell MariaDB which databases that user can access and what level of user they are, and we can do that with this command:

﻿	grant all privileges on WOLFGANG.* to wolfgang@localhost;

This gives the user wolfgang all privileges to the database wolfgang, for all tables (wolfgang.* = wolfgang database, all tables).

Whenever permissions are updated on a SQL database it’s a good idea to refresh the privileges in memory; you can do that with this command:

flush privileges;

Now let’s see if everything worked.  Enter this command to show all of the privileges for your user (again, replacing the username with the one you created) – take a snapshot and include it as before:

show grants for wolfgang@localhost;

Now let’s create some bioinformatics data tables in our SQL database.  In most cases you’ll be interacting with SQL programmatically (e.g. you can use python to read and write to SQL, to create databases, users, and tables, etc.) but in order to learn the proper syntax we’ll do a few here directly in the SQL command line interface.  

Before we can create tables we have to tell SQL which database we want to use.  We can do that with the following command (as before, substitute the name of the database you created above):

use WOLFGANG;

Notice that since I created my database in all caps I have to use that exact case when I invoke it in any commands!

With the database selected, let’s create a tables with the following commands:

CREATE TABLE Gene(gid INTEGER, name VARCHAR(20), annotation VARCHAR(50), PRIMARY KEY (gid));

This command creates a table, specifying it’s name as well the names of the fields – and the type of information they can include.  The basic syntax is:

Create table <tablename>(field1name field1type, field2name field2type,…,primary key(fieldname));

We aren’t going to go into all of the specifics, but you should know the different types of fields you can use (e.g. VARCHAR, INTEGER, REAL).  The primary key is the unique identifier for each row in the database and is used for indexing – for more information on this see: https://www.w3schools.com/sql/sql_primarykey.ASP.

You can use the following commands to verify that the tables were created properly:

show tables;

show columns from Gene;

Now let’s manually insert some data into the table.  We’ll use the same basic command that you would use if you were doing this programmatically:

insert into Gene VALUES(1, "1433E", "enzyme binding");

We are telling MariaDB to add values (not new fields or other things) to the table Gene, then within the parentheses we specify values for each of the defined fields.  Notice we do not reference the fields – you have to use the fields in order of creation, and use valid data types (e.g. INTEGER or VARCHAR).

The real power of a database, however, is querying the data – and to do an example of that we’ll need to add more data.  Let’s add two more genes to our Genes table:

insert into Gene VALUES(2, "PolA", "dna replication");

insert into Gene VALUES(3, "Apob", "enzyme binding");

insert into Gene VALUES(4, "PolB", "dna replication");

Now let’s create a second table that stores expression level data:

CREATE TABLE Expression(gid INTEGER, expression_level INTEGER, PRIMARY KEY (gid));

The new tables needs some expression data.  Notice that the gid (gene ID) is a field in common with the Gene table:

insert into Expression VALUES(1, 93);

insert into Expression VALUES(2, 107);

insert into Expression VALUES(3, 1701);

insert into Expression VALUES(4, 42);

Looking back at the Gene table you’ll see that we have two genes whose function is “enzyme binding” in our Gene table.  Using the two tables, let’s identify the name of the gene with the highest expression level:

SELECT name

FROM Gene JOIN Expression

ON Gene.gid = Expression.gid

WHERE expression_level = (select max(expression_level) from Expression);

If everything went well your query returned the name "Apob".  In order to make this query work, we had to join the two tables (Gene and Expression) on their common ID (the gid field) and so that we could find the name of the gene with the highest expression level.  But what if we wanted to narrow this down – let's find the DNA replication gene with the highest expression value:

SELECT name, expression_level

FROM Gene JOIN Expression

ON Gene.gid = Expression.gid

WHERE annotation="dna replication"

ORDER by expression_level DESC

LIMIT 1;

Here we are showing both the name and expression level, joining the two tables exactly the same way, but specifying that we want to look at only rows where the function (annotation) is DNA replication.  The tricky bit is we then sort the results by expression level in descending order (that's the 5th line) and limit the results to just the first result.  

For the last part of this exercise, perform these two steps and provide screen shots of the results:

 Add the following data to your tables.  You'll have to write your own SQL statements.

gid	name	annotation			gid	expression_level

5	Rpol	reverse transcriptase		5	05

6	tel	telomerase			6	120

 Now query the database.  List the genes in ascending order of expression level.



--- SOURCE: BIFS 614 HW5.docx ---

BIFS 614 – Data Structures & Algorithms

Homework 5:  Data Reduction:  PCA in R

To complete this homework you will have to use R and R-Studio.  If you do not already have them, they are available at no charge from the following links:

R download:  https://cran.r-project.org/bin

R-Studio download (choose the free desktop version): https://www.rstudio.com/products/rstudio/download/

In this exercise we will build on the R skills you learned in Week 2 to perform a data reduction analysis using PCA.  Recall that PCA tries to find the axes that contribute the most to the variation in the data – in this case, which axes best separate malignant from benign tumors. 

IN ORDER TO RECEIVE FULL CREDIT you are expected to comment each code block, describing what it does in detail.  You will have to research some of the R commands we use on your own so that you understand what they are doing.  You will receive 40 pts for running the exercise and 60 pts for answering the questions at the bottom of the assignment.  You must also submit your R script along with these answers!

Let's get started!

 Launch R Studio.  From the FILE menu choose NEW FILE -> RSCRIPT.

Using the hashtag comment indicator, put your name and the date at the top of your script. 

The PCA plots we will generate require the factoextra package, so you should install it the same way you have installed R packages previously.  Then add this line to your script to make sure it is loaded into memory for this analysis:

library(factoextra)

Now let's load the data set.  We will be using the Wisconsin Breast Cancer Data Set from the UCI Machine Learning repository; this dataset contains 30 columns of numerical data that has been extracted from breast cancer slide images.  In order to simplify the exercise, the data has been combined into a single .csv file (wcbd.csv) and is in the CLASS RESOURCES section of the classroom.  Download it and put it on your desktop, then load it into R:


wbcd <- read.csv('~/Desktop/wcbd.csv')

Also, you should understand what each of the commands here is doing – for example, what is read.csv doing?  

HINT:  if you type a command into the console window, RStudio will try to autocomplete the command and offer you guidance on what parameters the command needs – including a link to the built-in help.  Try this out by typing just the first three letters of the order command into the console.

Take a look at the data set.  Notice that the first two columns are not "data" but are descriptive information which we will exclude from the analysis.  That leaves 30 columns of data to reduce down to the most meaningful two which we can plot on a 2D plot.

Now let's proceed.  To make things easier we'll create a new matrix, removing the ID column from the original data set, and then add that column back in but as the row names:

wbcd.data <- wbcd[,c(2:32)]

row.names(wbcd.data) <- wbcd$id

To run the PCA we will invoke the prcomp function on the data:

wbcd.pca <- prcomp(wbcd.data[c(2:31)], center = TRUE, scale = TRUE)

We now have our principal components.  To display them you can enter this command:

summary(wbcd.pca)

Plotting the data is a simple matter of feeding the PCA into factoextra along with some parameters.  This works particularly well for this type of data; for general purpose plotting you should explore ggplot2, which is probably the most popular plotting package available for R.  Go ahead 

fviz_pca_ind(wbcd.pca, geom.ind = "point", pointshape = 21, 

             pointsize = 2, 

             fill.ind = wbcd$diagnosis, 

             col.ind = "black", 

             palette = "jco", 

             addEllipses = TRUE,

             label = "var",

             col.var = "black",

             repel = TRUE,

             legend.title = "Diagnosis") +

  ggtitle("2D PCA-plot from 30 feature dataset") +

  theme(plot.title = element_text(hjust = 0.5))

QUESTIONS TO ANSWER:

(10 pts) How many dimensions are in the original dataset?  How do you know?

(10 pts) What the the "center = true" and "scale = true" flags tell prcomp to do?

(10 pts) We used the statement wbcd.data[c(2:31)] in the PCA command.  What does this command do?  Why was it necessary?

(10 pts) How many principle components did you find in total?

(10 pts) How much variation do the first and second component combined account for?

(10 pts) Does the plot you generated show a good separation of malignant from benign cases?  Why or why not?



--- SOURCE: BIFS 614 HW6.docx ---

BIFS 614 – Data Structures & Algorithms

Homework 6:  Machine Learning in R

To complete this homework you will have to use R and R-Studio.  If you do not already have them, they are available at no charge from the following links:

R download:  https://cran.cnr.berkeley.edu/

R-Studio download (choose the free desktop version: https://www.rstudio.com/products/rstudio/download2/

In this exercise we will build on the R skills you learned in Week 2 as well as Homework 5 to create a neural network for breast cancer diagnosis.  We will be using the same data set you used in Homework 5 for the PCA analysis.

IN ORDER TO RECEIVE FULL CREDIT you are expected to comment each code block, describing what it does in detail.  You will have to research some of the R commands we use on your own so that you understand what they are doing.  You will receive 50 pts for running the exercise and 50 pts for answering the questions at the bottom of the assignment.  You must also submit your R script along with these answers!

Let's get started!

 Launch R Studio.  From the FILE menu choose NEW FILE -> RSCRIPT.

Using the hashtag comment indicator, put your name and the date at the top of your script. 

The neural network we will build requires the caret library, so be sure it is install and that you have the proper library load statement at the top of your script:

library(caret)

Now let's load the data set.  We will be using the same Wisconsin Breast Cancer Data Set from the UCI Machine Learning repository that we used for Homework 5, so you can use the same command to load it as before (make sure the file is still on your desktop first!):

wbcd <- read.csv('~/Desktop/wcbd.csv')

row.names(wbcd) <- wbcd$id

wbcd <- wbcd[,c(2:32)]

As before, you should understand what each of the commands here is doing so that you can include that information in a comment above the code!

HINT:  if you type a command into the console window, RStudio will try to autocomplete the command and offer you guidance on what parameters the command needs – including a link to the built-in help.  Try this out by typing just the first three letters of the order command into the console.

You should recall from the exercise we walked through in class how to create an index  of 80% of the data (in this case split by diagnosis) so that we can split the dataset:

index <- createDataPartition(wbcd$diagnosis, p=0.80, list=FALSE)

To create the training and validation data, we use the index (which represents 80% of the data) to partition the full data set:

validation <- wbcd[-index,]

training <- wbcd[index,]

Let's take a look at the distribution of classes (benign vs malignant) in the training data set:

percentage <- prop.table(table(training$diagnosis)) * 100

cbind(freq=table(training$diagnosis), percentage=percentage)

summary(training)

The next thing we're going to do is to save ourselves some time by setting variables for the "trControl" and "metric" flags used by each algorithm – just like we did in the iris dataset example we walked through previously:

control <- trainControl(method="cv", number=10)

metric <- "Accuracy"

Now let's test our four models and compare their accuracy.  Note that each "fit" command must be entered as a single line in R, even though this document wraps them to fit them on the page:

set.seed(7)

fit.lda <- train(diagnosis~., data=wbcd, method="lda", metric=metric, trControl=control)

fit.knn <- train(diagnosis~., data=wbcd, method="knn", metric=metric, trControl=control)

fit.svm <- train(diagnosis~., data=wbcd, method="svmRadial", metric=metric, trControl=control)

fit.rf <- train(diagnosis~., data=wbcd, method="rf", metric=metric, trControl=control)

results <- resamples(list(lda=fit.lda, knn=fit.knn, svm=fit.svm, rf=fit.rf))

summary(results)

 I'm going to use the lda-based model to do my predictions, but you should use whichever was the most accurate based on the above:

predictions <- predict(fit.lda, validation)

To test how well our predictions worked, we can generate a Confusion Matrix – but if we try that with the current datasets it won't work – the command confusionMatrix wants the variables to both be factors, and the field "diagnosis" in the validation dataset is a variable of type CHAR (this is a common sort of thing you will run into with R).  So we need to use a slightly different command than we did previously to make sure that the comparison is done with the diagnosis from validation as a factor:

confusionMatrix(predictions, as.factor(validation$diagnosis))

 For our last step we'll feed our model some patient data and see what our AI Cancer Diagnostic Neural Network thinks the diagnosis is.  The line that loads patient.data should be a single line, even though this document wraps it to make it fit:

patient.data <- read.csv(file="wcbd_new_patients.csv")

predict(fit.lda, patient.data)

QUESTIONS TO ANSWER:

(10 pts) When you looked at the distribution of the classes in your training data, what percentage beneign vs malignant did you have?

(10 pts) When you ran summary(results) for the various methods of nn model creation, which method was more accurate?  How could you tell? 

(10 pts) What was the accuracy of your neural network when you ran the validation data?  How can you tell?

(10 pts) Were any of the new patients from the "wbcd_new_patients" data diagnosed as malignant by your neural network?  How can you tell?

(10 pts) For your new patient analysis, what is the probability of a mis-diagnosis?