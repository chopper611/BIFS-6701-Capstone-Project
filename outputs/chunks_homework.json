[
  {
    "chunk_id": 0,
    "source": "BIFS 614 HW1.docx",
    "text": " BIFS 614 \u2013 Data Structures & Algorithms  Homework 1:   Algorithms & Data Structures  INSTRUCTIONS TO INSTRUCTOR  Choose any combination of these questions that add up to 100 points.  Change the questions every term to help cut down on cheating via internet.  Feel free to add additional questions; if you do please forward them to robert.rumpf@umgc.edu for inclusion in the master list.  Q (5 pts):  Write the line of code that would install the package ggplot2 in R.  Q (5 pts):  Write the line of "
  },
  {
    "chunk_id": 1,
    "source": "BIFS 614 HW1.docx",
    "text": " Write the line of code that would install the package ggplot2 in R.  Q (5 pts):  Write the line of code that defines the variable sequence as AATCGA in python.  Q (5 pts):  What is a CIGAR string?  How is it generated, and where will you find it?  Q (5 pts):  What is a BAI file?  Q (10 pts):  Compare and contrast VCF and GFF files.  Q (10 pts):  Compare & Contrast FASTA and FASTQ files.  Q (10 pts):  What are the required fields for a VCF file?  Q (10 pts):  What is BigO?  Q (10 pts):  What are"
  },
  {
    "chunk_id": 2,
    "source": "BIFS 614 HW1.docx",
    "text": "s):  What are the required fields for a VCF file?  Q (10 pts):  What is BigO?  Q (10 pts):  What are comment lines in an algorithm?  When should they be used?  Q (15 pts):  Describe the differences between TSV, CSV, and Excel.  Q (15 pts):  What are SAM and BAM files?  What are the differences between them files?  How are they generated?  Q (25 pts):  Convert the sample.fastq at https://culture-bioinformatics.org/UMGC/ to a fasta file and include it as an attachment to this homework.  Explain ho"
  },
  {
    "chunk_id": 3,
    "source": "BIFS 614 HW1.docx",
    "text": "oinformatics.org/UMGC/ to a fasta file and include it as an attachment to this homework.  Explain how you did this and what the differences are.  Was any information lost in the conversion?  Q (25 pts):  For a given algorithm, how will BigO change if the algorithm is run on different hardware (e.g. a faster processor)?  How will it change if a larger data set is run through the algorithm?  Explain.  Q (25 pts):  Using pseudocode, describe an algorithm that sums the values between 1 and N for a g"
  },
  {
    "chunk_id": 4,
    "source": "BIFS 614 HW1.docx",
    "text": ".  Q (25 pts):  Using pseudocode, describe an algorithm that sums the values between 1 and N for a given value of N.  What do you think the BigO will be for this algorithm and why?  Q (25 pts):  Using pseudocode, describe an algorithm that searches for all instances of \u201cATC\u201d in a given sequence.  What do you think the BigO will be for this algorithm and why?  Q (25 pts):  Using pseudocode, describe an algorithm that searches for palindromes of size 5 bases in a given sequence.  What do you think"
  },
  {
    "chunk_id": 5,
    "source": "BIFS 614 HW1.docx",
    "text": "e an algorithm that searches for palindromes of size 5 bases in a given sequence.  What do you think the BigO will be for this algorithm and why?    "
  },
  {
    "chunk_id": 6,
    "source": "BIFS 614 HW2.docx",
    "text": " BIFS 614 \u2013 Data Structures & Algorithms  Homework 2:   Indexing and Searching  INSTRUCTIONS TO INSTRUCTOR  Choose any combination of these questions that add up to 100 points.  Change the questions every term to help cut down on cheating via internet, including changing the sequences and or other variables.  Feel free to add additional questions; if you do please forward them to robert.rumpf@umgc.edu for inclusion in the master list.  Q (10 pts):  What is a relational database and why is it use"
  },
  {
    "chunk_id": 7,
    "source": "BIFS 614 HW2.docx",
    "text": ".edu for inclusion in the master list.  Q (10 pts):  What is a relational database and why is it useful in bioinformatics?  Q (10 pts):  Define kmer.  Provide 3 separate examples.  Q (10 pts):  Define a heuristic algorithm.  Q (10 pts):  If you have 25 taxa, how many possible rooted trees can you construct?  Q (20 pts):  Given the sequence AATTACAGGCGACAGATA find all kmers of size 3.  Create a table that displays their counts and positions.  Q (20 pts):  Construct a keyword tree for these three "
  },
  {
    "chunk_id": 8,
    "source": "BIFS 614 HW2.docx",
    "text": "le that displays their counts and positions.  Q (20 pts):  Construct a keyword tree for these three sequences:  AATACCAGAGCGAGCTTAGACG  AATACCGGACTAACGAATGATT   TCCAGACGACGTTTAAGCGACT  Q (20 pts):  Create a suffix tree for these three sequences:  AATACCAGAGCGAGCTTAGACG  AATACCGGACTAACGAATGATT   TCCAGACGACGTTTAAGCGACT  Q (20 pts):  Describe how a hash function works, using both descriptive text and a diagram.  Q (20 pts):  Describe how BLAST uses hash tables to speed up sequence searching.    "
  },
  {
    "chunk_id": 9,
    "source": "BIFS 614 HW2.docx",
    "text": " a diagram.  Q (20 pts):  Describe how BLAST uses hash tables to speed up sequence searching.    "
  },
  {
    "chunk_id": 10,
    "source": "BIFS 614 HW3.docx",
    "text": " BIFS 614 \u2013 Data Structures & Algorithms  Homework 3:  Data Parsing  INSTRUCTIONS   You have one week to complete this homework.  As always you must show all of your work (including LLM prompts and corrections as well as all outputs) for full credit.   Q1 (10 pts):  Describe grep and what it can be used for.  Be sure to describe what the following flags do:  -v, -F, -r  Q (10 pts):  Describe the function of the \u201c|\u201d operator and when you would use it.  Q (10 pts):  Describe sed and what it can be"
  },
  {
    "chunk_id": 11,
    "source": "BIFS 614 HW3.docx",
    "text": "unction of the \u201c|\u201d operator and when you would use it.  Q (10 pts):  Describe sed and what it can be used for.  Be sure to include an example.  Q (15 pts):  Describe awk and what it can be used for.  Be sure to include an example and describe what the following flags do:  -F, &&, and !.  Q (15 pts):  Write the full linux command you would use to convert string 1 below to string 2:  String1:  gene1gene2gene3  String2:  gene1gene2gene3gene4  Q (15 pts):  Write the linux command you would use to co"
  },
  {
    "chunk_id": 12,
    "source": "BIFS 614 HW3.docx",
    "text": "gene2gene3  String2:  gene1gene2gene3gene4  Q (15 pts):  Write the linux command you would use to combine 3 text files together into 1 large text file.  Explain what each step does.  Q (15 pts):  Write the linux command you would use to convert a FASTQ file to a FASTA file.  Explain what each step does.    "
  },
  {
    "chunk_id": 13,
    "source": "BIFS 614 HW4.docx",
    "text": " BIFS 614 \u2013 Data Structures & Algorithms  Homework 4:  SQL and MariaDB  For this homework you will be installing the MariaDB database application on your linux installation, creating a database and some tables.  In order to receive full credit for your work, you must show all commands used and all outputs.  10 pts:  Install MariaDB on your VirtualBox Linux system using the command line.  Use the following three commands to do so.  Each command must be entered as a SINGLE LINE in ubuntu.  Submit "
  },
  {
    "chunk_id": 14,
    "source": "BIFS 614 HW4.docx",
    "text": "ollowing three commands to do so.  Each command must be entered as a SINGLE LINE in ubuntu.  Submit a screen capture of your terminal window after each step for full credit:  sudo apt update  sudo apt install mariadb-server  sudo mysql_secure_installation  NOTE:  after the third command you will be asked to enter the current password for root.  You should use \u201cubuntu\u201d (without the quotes).  Then proceed to answer Y to all of the following questions.  10 pts:  Test whether or not your MariaDB ser"
  },
  {
    "chunk_id": 15,
    "source": "BIFS 614 HW4.docx",
    "text": "roceed to answer Y to all of the following questions.  10 pts:  Test whether or not your MariaDB server is up and running by entering the following command.  Submit a screen capture of the resulting output and press \u201cq\u201d to return to the command prompt:  sudo systemctl status mariadb  10 pts:  Log in to your SQL database.  As before submit a screen capture for full credit:  sudo mysql  10 pts:  At the MariaDB> prompt, create a new database using the following command, using your first name as the"
  },
  {
    "chunk_id": 16,
    "source": "BIFS 614 HW4.docx",
    "text": "the MariaDB> prompt, create a new database using the following command, using your first name as the name of your database (I\u2019ll us my name in the example):  create database WOLFGANG;  Notice the semicolon at the end of the command.  All SQL commands end with a semicolon.  Now type this command to list all of the existing databases:  show databases;  Notice that MariaDB (as any SQL database) has created other databases as support structure.  As before submit a screen capture of the resulting out"
  },
  {
    "chunk_id": 17,
    "source": "BIFS 614 HW4.docx",
    "text": "reated other databases as support structure.  As before submit a screen capture of the resulting output.   Now let\u2019s create a user for the new database using the following command (again, I\u2019m using the database WOLFGANG; you should use the one you created above, and use your name for the user, not mine):  \ufeffcreate user 'wolfgang'@localhost identified by 'mypassword';  \tLet\u2019s make sure the user was created by entering this command:  select user from mysql.user;  This command looks at the mysql dat"
  },
  {
    "chunk_id": 18,
    "source": "BIFS 614 HW4.docx",
    "text": "created by entering this command:  select user from mysql.user;  This command looks at the mysql database (which you\u2019ll recall was listed when you listed the databases) and specifically shows the contents of the table user \u2013 which is why we used mysql.user above; it specifies database.table.  MariaDB now has a user, but that user has no privileges.  We need to specifically tell MariaDB which databases that user can access and what level of user they are, and we can do that with this command:  \ufeff\t"
  },
  {
    "chunk_id": 19,
    "source": "BIFS 614 HW4.docx",
    "text": "ases that user can access and what level of user they are, and we can do that with this command:  \ufeff\tgrant all privileges on WOLFGANG.* to wolfgang@localhost;  This gives the user wolfgang all privileges to the database wolfgang, for all tables (wolfgang.* = wolfgang database, all tables).  Whenever permissions are updated on a SQL database it\u2019s a good idea to refresh the privileges in memory; you can do that with this command:  flush privileges;  Now let\u2019s see if everything worked.  Enter this c"
  },
  {
    "chunk_id": 20,
    "source": "BIFS 614 HW4.docx",
    "text": "can do that with this command:  flush privileges;  Now let\u2019s see if everything worked.  Enter this command to show all of the privileges for your user (again, replacing the username with the one you created) \u2013 take a snapshot and include it as before:  show grants for wolfgang@localhost;  Now let\u2019s create some bioinformatics data tables in our SQL database.  In most cases you\u2019ll be interacting with SQL programmatically (e.g. you can use python to read and write to SQL, to create databases, users"
  },
  {
    "chunk_id": 21,
    "source": "BIFS 614 HW4.docx",
    "text": "h SQL programmatically (e.g. you can use python to read and write to SQL, to create databases, users, and tables, etc.) but in order to learn the proper syntax we\u2019ll do a few here directly in the SQL command line interface.    Before we can create tables we have to tell SQL which database we want to use.  We can do that with the following command (as before, substitute the name of the database you created above):  use WOLFGANG;  Notice that since I created my database in all caps I have to use t"
  },
  {
    "chunk_id": 22,
    "source": "BIFS 614 HW4.docx",
    "text": " created above):  use WOLFGANG;  Notice that since I created my database in all caps I have to use that exact case when I invoke it in any commands!  With the database selected, let\u2019s create a tables with the following commands:  CREATE TABLE Gene(gid INTEGER, name VARCHAR(20), annotation VARCHAR(50), PRIMARY KEY (gid));  This command creates a table, specifying it\u2019s name as well the names of the fields \u2013 and the type of information they can include.  The basic syntax is:  Create table <tablenam"
  },
  {
    "chunk_id": 23,
    "source": "BIFS 614 HW4.docx",
    "text": "fields \u2013 and the type of information they can include.  The basic syntax is:  Create table <tablename>(field1name field1type, field2name field2type,\u2026,primary key(fieldname));  We aren\u2019t going to go into all of the specifics, but you should know the different types of fields you can use (e.g. VARCHAR, INTEGER, REAL).  The primary key is the unique identifier for each row in the database and is used for indexing \u2013 for more information on this see: https://www.w3schools.com/sql/sql_primarykey.ASP. "
  },
  {
    "chunk_id": 24,
    "source": "BIFS 614 HW4.docx",
    "text": " for indexing \u2013 for more information on this see: https://www.w3schools.com/sql/sql_primarykey.ASP.  You can use the following commands to verify that the tables were created properly:  show tables;  show columns from Gene;  Now let\u2019s manually insert some data into the table.  We\u2019ll use the same basic command that you would use if you were doing this programmatically:  insert into Gene VALUES(1, \"1433E\", \"enzyme binding\");  We are telling MariaDB to add values (not new fields or other things) to"
  },
  {
    "chunk_id": 25,
    "source": "BIFS 614 HW4.docx",
    "text": "1433E\", \"enzyme binding\");  We are telling MariaDB to add values (not new fields or other things) to the table Gene, then within the parentheses we specify values for each of the defined fields.  Notice we do not reference the fields \u2013 you have to use the fields in order of creation, and use valid data types (e.g. INTEGER or VARCHAR).  The real power of a database, however, is querying the data \u2013 and to do an example of that we\u2019ll need to add more data.  Let\u2019s add two more genes to our Genes tab"
  },
  {
    "chunk_id": 26,
    "source": "BIFS 614 HW4.docx",
    "text": "and to do an example of that we\u2019ll need to add more data.  Let\u2019s add two more genes to our Genes table:  insert into Gene VALUES(2, \"PolA\", \"dna replication\");  insert into Gene VALUES(3, \"Apob\", \"enzyme binding\");  insert into Gene VALUES(4, \"PolB\", \"dna replication\");  Now let\u2019s create a second table that stores expression level data:  CREATE TABLE Expression(gid INTEGER, expression_level INTEGER, PRIMARY KEY (gid));  The new tables needs some expression data.  Notice that the gid (gene ID) is"
  },
  {
    "chunk_id": 27,
    "source": "BIFS 614 HW4.docx",
    "text": "R, PRIMARY KEY (gid));  The new tables needs some expression data.  Notice that the gid (gene ID) is a field in common with the Gene table:  insert into Expression VALUES(1, 93);  insert into Expression VALUES(2, 107);  insert into Expression VALUES(3, 1701);  insert into Expression VALUES(4, 42);  Looking back at the Gene table you\u2019ll see that we have two genes whose function is \u201cenzyme binding\u201d in our Gene table.  Using the two tables, let\u2019s identify the name of the gene with the highest expre"
  },
  {
    "chunk_id": 28,
    "source": "BIFS 614 HW4.docx",
    "text": "in our Gene table.  Using the two tables, let\u2019s identify the name of the gene with the highest expression level:  SELECT name  FROM Gene JOIN Expression  ON Gene.gid = Expression.gid  WHERE expression_level = (select max(expression_level) from Expression);  If everything went well your query returned the name \"Apob\".  In order to make this query work, we had to join the two tables (Gene and Expression) on their common ID (the gid field) and so that we could find the name of the gene with the hig"
  },
  {
    "chunk_id": 29,
    "source": "BIFS 614 HW4.docx",
    "text": "sion) on their common ID (the gid field) and so that we could find the name of the gene with the highest expression level.  But what if we wanted to narrow this down \u2013 let's find the DNA replication gene with the highest expression value:  SELECT name, expression_level  FROM Gene JOIN Expression  ON Gene.gid = Expression.gid  WHERE annotation=\"dna replication\"  ORDER by expression_level DESC  LIMIT 1;  Here we are showing both the name and expression level, joining the two tables exactly the sam"
  },
  {
    "chunk_id": 30,
    "source": "BIFS 614 HW4.docx",
    "text": "T 1;  Here we are showing both the name and expression level, joining the two tables exactly the same way, but specifying that we want to look at only rows where the function (annotation) is DNA replication.  The tricky bit is we then sort the results by expression level in descending order (that's the 5th line) and limit the results to just the first result.    For the last part of this exercise, perform these two steps and provide screen shots of the results:   Add the following data to your t"
  },
  {
    "chunk_id": 31,
    "source": "BIFS 614 HW4.docx",
    "text": " perform these two steps and provide screen shots of the results:   Add the following data to your tables.  You'll have to write your own SQL statements.  gid\tname\tannotation\t\t\tgid\texpression_level  5\tRpol\treverse transcriptase\t\t5\t05  6\ttel\ttelomerase\t\t\t6\t120   Now query the database.  List the genes in ascending order of expression level.    "
  },
  {
    "chunk_id": 32,
    "source": "BIFS 614 HW5.docx",
    "text": " BIFS 614 \u2013 Data Structures & Algorithms  Homework 5:  Data Reduction:  PCA in R  To complete this homework you will have to use R and R-Studio.  If you do not already have them, they are available at no charge from the following links:  R download:  https://cran.r-project.org/bin  R-Studio download (choose the free desktop version): https://www.rstudio.com/products/rstudio/download/  In this exercise we will build on the R skills you learned in Week 2 to perform a data reduction analysis using "
  },
  {
    "chunk_id": 33,
    "source": "BIFS 614 HW5.docx",
    "text": "cise we will build on the R skills you learned in Week 2 to perform a data reduction analysis using PCA.  Recall that PCA tries to find the axes that contribute the most to the variation in the data \u2013 in this case, which axes best separate malignant from benign tumors.   IN ORDER TO RECEIVE FULL CREDIT you are expected to comment each code block, describing what it does in detail.  You will have to research some of the R commands we use on your own so that you understand what they are doing.  Yo"
  },
  {
    "chunk_id": 34,
    "source": "BIFS 614 HW5.docx",
    "text": "o research some of the R commands we use on your own so that you understand what they are doing.  You will receive 40 pts for running the exercise and 60 pts for answering the questions at the bottom of the assignment.  You must also submit your R script along with these answers!  Let's get started!   Launch R Studio.  From the FILE menu choose NEW FILE -> RSCRIPT.  Using the hashtag comment indicator, put your name and the date at the top of your script.   The PCA plots we will generate require"
  },
  {
    "chunk_id": 35,
    "source": "BIFS 614 HW5.docx",
    "text": "ator, put your name and the date at the top of your script.   The PCA plots we will generate require the factoextra package, so you should install it the same way you have installed R packages previously.  Then add this line to your script to make sure it is loaded into memory for this analysis:  library(factoextra)  Now let's load the data set.  We will be using the Wisconsin Breast Cancer Data Set from the UCI Machine Learning repository; this dataset contains 30 columns of numerical data that"
  },
  {
    "chunk_id": 36,
    "source": "BIFS 614 HW5.docx",
    "text": "et from the UCI Machine Learning repository; this dataset contains 30 columns of numerical data that has been extracted from breast cancer slide images.  In order to simplify the exercise, the data has been combined into a single .csv file (wcbd.csv) and is in the CLASS RESOURCES section of the classroom.  Download it and put it on your desktop, then load it into R:   wbcd <- read.csv('~/Desktop/wcbd.csv')  Also, you should understand what each of the commands here is doing \u2013 for example, what i"
  },
  {
    "chunk_id": 37,
    "source": "BIFS 614 HW5.docx",
    "text": "cbd.csv')  Also, you should understand what each of the commands here is doing \u2013 for example, what is read.csv doing?    HINT:  if you type a command into the console window, RStudio will try to autocomplete the command and offer you guidance on what parameters the command needs \u2013 including a link to the built-in help.  Try this out by typing just the first three letters of the order command into the console.  Take a look at the data set.  Notice that the first two columns are not \"data\" but are"
  },
  {
    "chunk_id": 38,
    "source": "BIFS 614 HW5.docx",
    "text": "the console.  Take a look at the data set.  Notice that the first two columns are not \"data\" but are descriptive information which we will exclude from the analysis.  That leaves 30 columns of data to reduce down to the most meaningful two which we can plot on a 2D plot.  Now let's proceed.  To make things easier we'll create a new matrix, removing the ID column from the original data set, and then add that column back in but as the row names:  wbcd.data <- wbcd[,c(2:32)]  row.names(wbcd.data) <"
  },
  {
    "chunk_id": 39,
    "source": "BIFS 614 HW5.docx",
    "text": "n add that column back in but as the row names:  wbcd.data <- wbcd[,c(2:32)]  row.names(wbcd.data) <- wbcd$id  To run the PCA we will invoke the prcomp function on the data:  wbcd.pca <- prcomp(wbcd.data[c(2:31)], center = TRUE, scale = TRUE)  We now have our principal components.  To display them you can enter this command:  summary(wbcd.pca)  Plotting the data is a simple matter of feeding the PCA into factoextra along with some parameters.  This works particularly well for this type of data; "
  },
  {
    "chunk_id": 40,
    "source": "BIFS 614 HW5.docx",
    "text": "CA into factoextra along with some parameters.  This works particularly well for this type of data; for general purpose plotting you should explore ggplot2, which is probably the most popular plotting package available for R.  Go ahead   fviz_pca_ind(wbcd.pca, geom.ind = \"point\", pointshape = 21,                pointsize = 2,                fill.ind = wbcd$diagnosis,                col.ind = \"black\",                palette = \"jco\",                addEllipses = TRUE,               label = \"var\", "
  },
  {
    "chunk_id": 41,
    "source": "BIFS 614 HW5.docx",
    "text": "k\",                palette = \"jco\",                addEllipses = TRUE,               label = \"var\",               col.var = \"black\",               repel = TRUE,               legend.title = \"Diagnosis\") +    ggtitle(\"2D PCA-plot from 30 feature dataset\") +    theme(plot.title = element_text(hjust = 0.5))  QUESTIONS TO ANSWER:  (10 pts) How many dimensions are in the original dataset?  How do you know?  (10 pts) What the the \"center = true\" and \"scale = true\" flags tell prcomp to do?  (10 pts) We"
  },
  {
    "chunk_id": 42,
    "source": "BIFS 614 HW5.docx",
    "text": "now?  (10 pts) What the the \"center = true\" and \"scale = true\" flags tell prcomp to do?  (10 pts) We used the statement wbcd.data[c(2:31)] in the PCA command.  What does this command do?  Why was it necessary?  (10 pts) How many principle components did you find in total?  (10 pts) How much variation do the first and second component combined account for?  (10 pts) Does the plot you generated show a good separation of malignant from benign cases?  Why or why not?    "
  },
  {
    "chunk_id": 43,
    "source": "BIFS 614 HW5.docx",
    "text": " a good separation of malignant from benign cases?  Why or why not?    "
  },
  {
    "chunk_id": 44,
    "source": "BIFS 614 HW6.docx",
    "text": " BIFS 614 \u2013 Data Structures & Algorithms  Homework 6:  Machine Learning in R  To complete this homework you will have to use R and R-Studio.  If you do not already have them, they are available at no charge from the following links:  R download:  https://cran.cnr.berkeley.edu/  R-Studio download (choose the free desktop version: https://www.rstudio.com/products/rstudio/download2/  In this exercise we will build on the R skills you learned in Week 2 as well as Homework 5 to create a neural networ"
  },
  {
    "chunk_id": 45,
    "source": "BIFS 614 HW6.docx",
    "text": " we will build on the R skills you learned in Week 2 as well as Homework 5 to create a neural network for breast cancer diagnosis.  We will be using the same data set you used in Homework 5 for the PCA analysis.  IN ORDER TO RECEIVE FULL CREDIT you are expected to comment each code block, describing what it does in detail.  You will have to research some of the R commands we use on your own so that you understand what they are doing.  You will receive 50 pts for running the exercise and 50 pts f"
  },
  {
    "chunk_id": 46,
    "source": "BIFS 614 HW6.docx",
    "text": "t you understand what they are doing.  You will receive 50 pts for running the exercise and 50 pts for answering the questions at the bottom of the assignment.  You must also submit your R script along with these answers!  Let's get started!   Launch R Studio.  From the FILE menu choose NEW FILE -> RSCRIPT.  Using the hashtag comment indicator, put your name and the date at the top of your script.   The neural network we will build requires the caret library, so be sure it is install and that yo"
  },
  {
    "chunk_id": 47,
    "source": "BIFS 614 HW6.docx",
    "text": "   The neural network we will build requires the caret library, so be sure it is install and that you have the proper library load statement at the top of your script:  library(caret)  Now let's load the data set.  We will be using the same Wisconsin Breast Cancer Data Set from the UCI Machine Learning repository that we used for Homework 5, so you can use the same command to load it as before (make sure the file is still on your desktop first!):  wbcd <- read.csv('~/Desktop/wcbd.csv')  row.name"
  },
  {
    "chunk_id": 48,
    "source": "BIFS 614 HW6.docx",
    "text": "ke sure the file is still on your desktop first!):  wbcd <- read.csv('~/Desktop/wcbd.csv')  row.names(wbcd) <- wbcd$id  wbcd <- wbcd[,c(2:32)]  As before, you should understand what each of the commands here is doing so that you can include that information in a comment above the code!  HINT:  if you type a command into the console window, RStudio will try to autocomplete the command and offer you guidance on what parameters the command needs \u2013 including a link to the built-in help.  Try this ou"
  },
  {
    "chunk_id": 49,
    "source": "BIFS 614 HW6.docx",
    "text": " guidance on what parameters the command needs \u2013 including a link to the built-in help.  Try this out by typing just the first three letters of the order command into the console.  You should recall from the exercise we walked through in class how to create an index  of 80% of the data (in this case split by diagnosis) so that we can split the dataset:  index <- createDataPartition(wbcd$diagnosis, p=0.80, list=FALSE)  To create the training and validation data, we use the index (which represents"
  },
  {
    "chunk_id": 50,
    "source": "BIFS 614 HW6.docx",
    "text": " p=0.80, list=FALSE)  To create the training and validation data, we use the index (which represents 80% of the data) to partition the full data set:  validation <- wbcd[-index,]  training <- wbcd[index,]  Let's take a look at the distribution of classes (benign vs malignant) in the training data set:  percentage <- prop.table(table(training$diagnosis)) * 100  cbind(freq=table(training$diagnosis), percentage=percentage)  summary(training)  The next thing we're going to do is to save ourselves so"
  },
  {
    "chunk_id": 51,
    "source": "BIFS 614 HW6.docx",
    "text": " percentage=percentage)  summary(training)  The next thing we're going to do is to save ourselves some time by setting variables for the \"trControl\" and \"metric\" flags used by each algorithm \u2013 just like we did in the iris dataset example we walked through previously:  control <- trainControl(method=\"cv\", number=10)  metric <- \"Accuracy\"  Now let's test our four models and compare their accuracy.  Note that each \"fit\" command must be entered as a single line in R, even though this document wraps "
  },
  {
    "chunk_id": 52,
    "source": "BIFS 614 HW6.docx",
    "text": "Note that each \"fit\" command must be entered as a single line in R, even though this document wraps them to fit them on the page:  set.seed(7)  fit.lda <- train(diagnosis~., data=wbcd, method=\"lda\", metric=metric, trControl=control)  fit.knn <- train(diagnosis~., data=wbcd, method=\"knn\", metric=metric, trControl=control)  fit.svm <- train(diagnosis~., data=wbcd, method=\"svmRadial\", metric=metric, trControl=control)  fit.rf <- train(diagnosis~., data=wbcd, method=\"rf\", metric=metric, trControl=co"
  },
  {
    "chunk_id": 53,
    "source": "BIFS 614 HW6.docx",
    "text": "trControl=control)  fit.rf <- train(diagnosis~., data=wbcd, method=\"rf\", metric=metric, trControl=control)  results <- resamples(list(lda=fit.lda, knn=fit.knn, svm=fit.svm, rf=fit.rf))  summary(results)   I'm going to use the lda-based model to do my predictions, but you should use whichever was the most accurate based on the above:  predictions <- predict(fit.lda, validation)  To test how well our predictions worked, we can generate a Confusion Matrix \u2013 but if we try that with the current datas"
  },
  {
    "chunk_id": 54,
    "source": "BIFS 614 HW6.docx",
    "text": "r predictions worked, we can generate a Confusion Matrix \u2013 but if we try that with the current datasets it won't work \u2013 the command confusionMatrix wants the variables to both be factors, and the field \"diagnosis\" in the validation dataset is a variable of type CHAR (this is a common sort of thing you will run into with R).  So we need to use a slightly different command than we did previously to make sure that the comparison is done with the diagnosis from validation as a factor:  confusionMatr"
  },
  {
    "chunk_id": 55,
    "source": "BIFS 614 HW6.docx",
    "text": "make sure that the comparison is done with the diagnosis from validation as a factor:  confusionMatrix(predictions, as.factor(validation$diagnosis))   For our last step we'll feed our model some patient data and see what our AI Cancer Diagnostic Neural Network thinks the diagnosis is.  The line that loads patient.data should be a single line, even though this document wraps it to make it fit:  patient.data <- read.csv(file=\"wcbd_new_patients.csv\")  predict(fit.lda, patient.data)  QUESTIONS TO AN"
  },
  {
    "chunk_id": 56,
    "source": "BIFS 614 HW6.docx",
    "text": "ient.data <- read.csv(file=\"wcbd_new_patients.csv\")  predict(fit.lda, patient.data)  QUESTIONS TO ANSWER:  (10 pts) When you looked at the distribution of the classes in your training data, what percentage beneign vs malignant did you have?  (10 pts) When you ran summary(results) for the various methods of nn model creation, which method was more accurate?  How could you tell?   (10 pts) What was the accuracy of your neural network when you ran the validation data?  How can you tell?  (10 pts) W"
  },
  {
    "chunk_id": 57,
    "source": "BIFS 614 HW6.docx",
    "text": "the accuracy of your neural network when you ran the validation data?  How can you tell?  (10 pts) Were any of the new patients from the \"wbcd_new_patients\" data diagnosed as malignant by your neural network?  How can you tell?  (10 pts) For your new patient analysis, what is the probability of a mis-diagnosis? "
  }
]